# 2000202-200208 论文阅读总结

[TOC]

## 论文列表

1. Uncertainty Autoencoders: Learning Compressed Representations via Variational Information Maximization:  UAE

2. [Hyperspectral Image Reconstruction Using a Deep Spatial-Spectral Prior](https://github.com/Theodore-PKU/paper-notes/blob/master/hyperspectral-image-reconstruction-using-a-deep-spatial-spectral-prior-2019-200202.md): Hyperspectral Image

3. [Eﬃcient Structurally-Strengthened Generative Adversarial Network for MRI Reconstruction](https://github.com/Theodore-PKU/paper-notes/blob/master/efficient-structurally-strengthened-generative-adversarial-network-for-mri-reconstruction-2019-200202.md): MRI

4. [Dynamic MRI using model‐based deep learning and SToRM priors: MoDL‐SToRM](https://github.com/Theodore-PKU/paper-notes/blob/master/dynamic-mri-using-model-based-deep-learning-and-storm-priors-modl-storm-2019-200102.md): MRI

5. [Spatio-Temporal Deep Learning-Based Undersampling Artefact Reduction for 2D Radial Cine MRI with Limited Training Data](https://github.com/Theodore-PKU/paper-notes/blob/master/spatio-temporal-deep-learning-based-undersampling-artefact-redunction-for-2d-radial-cine-mri-with-limited-training-data-2019-200202.md): MRI

6. [One-dimensional Deep Image Prior for Time Series Inverse Problems](https://github.com/Theodore-PKU/paper-notes/blob/master/one-dimensional-deep-image-prior-for-time-series-inverse-problems-2019-200202.md): DIP

7. [Sinogram interpolation for sparse-view micro-CT with deep learning neural network](https://github.com/Theodore-PKU/paper-notes/blob/master/sinogram-interpolation-for-sparse-view-micro-ct-with-deep-learning-neural-network-2019-200202.md): CT

8. [An improved method for single image super-resolution based on deep learning](https://github.com/Theodore-PKU/paper-notes/blob/master/an-improved-method-for-single-image-super-resolution-based-on-deep-learning-2019-200203.md): SR

9. Deep MR Fingerprinting with total-variation and low-rank subspace priors

10. [Alternating Phase Projected Gradient Descent with Generative Priors for Solving Compressive Phase Retrieval](https://github.com/Theodore-PKU/paper-notes/blob/master/alternating-phase-projected-gradient-descent-with-generative-priors-for-solving-compressive-phase-retrieval-2019-200203.md): phase retrieval

11. Deep learning for low-dose CT：CT

12. Information-Theoretic Lower Bounds for Compressive Sensing with Generative Models：theory

13. [Deep Residual Dense U-Net for Resolution Enhancement in Accelerated MRI Acquisition](https://github.com/Theodore-PKU/paper-notes/blob/master/deep-residual-dense-u-net-for-resolution-enhancement-in-accelerated-mri-acquisition-2019-200203.md): MRI

14. [GrappaNet: Combining Parallel Imaging with Deep Learning for Multi-Coil MRI Reconstruction](https://github.com/Theodore-PKU/paper-notes/blob/master/grappanet-combining-parallel-imaging-with-deep-learning-for-multi-coil-mri-reconstruction-2019-200203.md): MRI

15. [Neumann Networks for Linear Inverse Problems in Imaging](https://github.com/Theodore-PKU/paper-notes/blob/master/neumann-networks-for-linear-inverse-problems-in-imaging-2019-200204.md): inverse problem

16. Regularizing linear inverse problems with convolutional neural networks：theory

17. [NETT Regularization for Compressed Sensing Photoacoustic Tomography](https://github.com/Theodore-PKU/paper-notes/blob/master/nett-regularization-for-compressed-sensing-photoacoustic-tomography-2019-200204.md): PAT

18. Deep Learning for Inverse Problems: Bounds and Regularizers：regularization of deep learning

19. [A Very Deep Densely Connected Network for Compressed Sensing MRI](https://github.com/Theodore-PKU/paper-notes/blob/master/a-vert-deep-densely-connected-network-for-compressed-sensing-mri-2019-200204.md): 

20. [Learning Sub-Sampling and Signal Recovery with Applications in Ultrasound Imaging](https://github.com/Theodore-PKU/paper-notes/blob/master/learning-sub-sampling-and-signal-recovery-with-applications-in-ultrasound-image-2019-200204.md): 

21. SURE-TISTA: A Signal Recovery Network For Compressed Sensing: CS

22. [Error Resilient Deep Compressive Sensing](https://github.com/Theodore-PKU/paper-notes/blob/master/error-resilient-deep-compressive-sensing-2019-200205.md): 

23. [Low Shot Learning with Untrained Neural Networks for Imaging Inverse Problems](https://github.com/Theodore-PKU/paper-notes/blob/master/low-shot-learning-with-untrained-neural-networks-for-imaging-inverse-problems-2019-200205.md): inverse problem

24. [A Theoretically Guaranteed Deep Optimization Framework for Robust Compressive Sensing MRI](https://github.com/Theodore-PKU/paper-notes/blob/master/a-theoretically-guaranteed-deep-optimization-framework-for-robust-compressive-sensing-mri-2019-200205.md): MRI

25. Accelerated Projection Reconstruction MR imaging using Deep Residual Learning: MRI

26. Surﬁng: Iterative Optimization Over Incrementally Trained Deep Networks: optimization

27. Non-Learning based Deep Parallel MRI Reconstruction (NLDpMRI): MRI

28. Assessment of the generalization of learned image reconstruction and the potential for transfer learning: MRI

29. [Deep Decomposition Learning for Inverse Imaging Problems](https://github.com/Theodore-PKU/paper-notes/blob/master/deep-decomposition-learning-for-inverse-imaging-problems-2019-200206.md): inverse problem

30. Real-time Cardiovascular MR with Spatio-temporal Artifact Suppression using Deep Learning - Proof of Concept in Congenital Heart Disease: MRI

31. Robust contrast-transfer-function phase retrieval via flexible deep learning networks：phase retrieval

32. [DeepcomplexMRI: Exploiting deep residual network for fast parallel MR imaging with complex convolution](https://github.com/Theodore-PKU/paper-notes/blob/master/deepcomplexmri-exploiting-deep-residual-network-for-fast-parallel-mr-imaging-with-complex-2019-200206.md): MRI

33. [Variational Deep Learning for Low-dose Computed Tomography](https://github.com/Theodore-PKU/paper-notes/blob/master/variational-deep-learning-for-low-dose-computed-tomography-2018-200206.md): CT 

34. [Learning from our neighbours: a novel approach on sinogram completion using bin-sharing and deep learning to reconstruct high quality 4DCBCT](https://github.com/Theodore-PKU/paper-notes/blob/master/learning-from-our-neighbours-a-novel-approach-on-sinogram-completion-using-bin-sharing-and-deep-learning-to-reconstruct-high-quality-4dcbct-2019-200206.md): CT

35. Deep Compressed Sensing: CS

36. [pISTA-SENSE-ResNet for Parallel MRI Reconstruction](https://github.com/Theodore-PKU/paper-notes/blob/master/pista-sense-resnet-for-parallel-mri-reconstruction-2019-200207.md): MRI

37. [λ-net: Reconstruct Hyperspectral Images from a Snapshot Measurement](https://github.com/Theodore-PKU/paper-notes/blob/master/λ-net-reconstruct-hyperspectral-images-from-a-snapshot-measurement-2019-200207.md): hyper spectral image

38. [IFR-Net: Iterative Feature Refinement Network for Compressed Sensing MRI](https://github.com/Theodore-PKU/paper-notes/blob/master/Ifr-net-iterative-feature-refinement-network-for-compressed-sensing-mri-2019-200208.md): MRI

39. [Learning Priors in High-frequency Domain for Inverse Imaging Reconstruction](https://github.com/Theodore-PKU/paper-notes/blob/master/learning-priors-in-high-frequency-domain-for-inverse-imaging-reconstruction-2019-200208.md): inverse problem

40. On the existence of stable and accurate neural networks for image reconstruction

41. Insights into Learning-Based MRI Reconstruction: Overview

42. Learning the Weight Matrix for Sparsity Averaging in Compressive Imaging: CS

43. Learning to solve inverse problems using Wasserstein loss

44. [fastMRI: An Open Dataset and Benchmarks for Accelerated MRI](https://github.com/Theodore-PKU/paper-notes/blob/master/fastmri-an-open-dataset-and-benchmarks-for-accelerated-mri-2018-200208.md): Dataset 

    

## Ideas from papers

- 在 Spatio-temporal 的 3D 重建问题中，使用空间和时间两个方向的的 slice 作为输入，可以增加很多训练数据（Spatio-Temporal Deep Learning-Based Undersampling Artefact Reduction for 2D Radial Cine MRI with Limited Training Data）
- 把重建信号分解成正交两个部分（Deep Decomposition Learning for Inverse Imaging Problems）



## 个人思考

- 如何解释 DIP，也许网络结构起到的是一个超定方程的框架，DIP 的有效要从函数表示的角度入手，猜测 DIP 的参数就是在求解一个类似线性方程的系统，只不过是局部、非线性的。在每个局部都是一个超定方程（而且超定的程度很高），多个超定方程构成一个超定程度较低的方程），这样以来，对于随机的输入，总是可以找到一组解。为什么 DIP 一般比较快的生成低频部分或简单部分，应该就是因为 SGD 算法在求解超定方程时的特性吧，网络结构的正则项也许只是提供了求解系统的框架。重建信号本身的性质和超定方程求解过程中达成了某种一致性，才会让DIP的效果好。如果网络结构不同，对应不同的求解系统，那么对于需要重建的信号，效果可能就不一样。如果按照这样的解释，也可以用一些较为简单的模型来证明一些结论，也可以设计各种实验来验证，比如设计一些反常的信号来重建。这个思路是否可以用来解释 GAN 等其他方法？
- 分片线性函数和 relu 的关系，激活值为0是不是某种函数表示的控制。任何一个1维的分片函数都可以用多层的带 relu 的线性函数来表示，每一层都可以增加分片的粒度。<img src="https://raw.githubusercontent.com/Theodore-PKU/pictures/master/%E6%88%AA%E5%B1%8F2020-02-04%E4%B8%8B%E5%8D%884.38.47.png"/>



讨论：

noise2noise 和 DIP 的关系

生成模型

