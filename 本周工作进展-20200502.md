# 本周工作进展-20200502

[TOC]

本周的工作主要是利用 *A Framework for the construction of upper bounds on the number of aﬃne linear regions of ReLU feed-forward neural networks* 这篇文章提出的若干概念和计算框架，将我们提出的区域划分的上界的计算用矩阵形式表示，并且拓展到其他更为复杂的网络结构中。

## A Framework for the construction of upper bounds on the number of aﬃne linear regions of ReLU feed-forward neural networks 中的相关概念和记号

### 记号

$h$：$\mathbb{R}^{n} \rightarrow \mathbb{R}^{n^{\prime}}, x \mapsto\left(\sigma\left(\left\langle x, w_{i}^{(h)}\right\rangle+b_{i}^{(h)}\right)\right)_{i \in\left\{1, \ldots, n^{\prime}\right\}}$，线性层 + ReLU 层

$RL(n,n'):=\left\{h: \mathbb{R}^{n} \rightarrow \mathbb{R}^{n^{\prime}} | h \text { is a ReLU Layer function }\right\}$，$h$ 的集合，也就是单层网络

$H^{h}_i:=\left\{x \in \mathbb{R}^{n} |\left\langle x, w_{i}^{(h)}\right\rangle+b_{i}^{(h)}=0\right\} \subset \mathbb{R}^{n} \quad \text { for } i \in\left\{1, \ldots, n^{\prime}\right\}$，$h$ 的第 $i$ 个超平面

$\mathrm{RL}\left(n_{0}, \mathbf{n}\right):=\operatorname{RL}\left(n_{0}, n_{1}\right) \times \operatorname{RL}\left(n_{1}, n_{2}\right) \times \cdots \times \operatorname{RL}\left(n_{L-1}, n_{L}\right), \mathbf{n}=\left(n_{1}, \ldots, n_{L}\right) \in \mathbb{N}_{+}^{L}$，$L$ 层网络的各层函数集合，也可以表示为 $\mathbf{h}=\left(h_{1}, \ldots, h_{L}\right) \in \mathrm{RL}\left(n_{0}, \mathbf{n}\right)$，网络记为：
$$
f_{\mathbf{h}}:\left\{\begin{array}{ll}\mathbb{R}^{n_{0}} & \rightarrow \mathbb{R}^{n_{L}} \\x & \mapsto h_{L} \circ \cdots \circ h_{1}(x)\end{array}\right.
$$

### 定义

$S_{h}(x) \in\{0,1\}^{n^{\prime}}$ 表示输入 $x$ 在 $h$ 运算中的激活 pattern

$R_{h}(s):=\left\{x \in \mathbb{R}^{n} | S_{h}(x)=s\right\}$：单层网络某一激活 pattern 对应的输入空间的区域

$\mathcal{S}_{h}:=\left\{S_{h}(x) \in\{0,1\}^{n^{\prime}} | x \in \mathbb{R}^{n}\right\}$：$h$ 运算下所有可能的激活 pattern

$S_{\mathbf{h}}(x)=\left(S_{h_{1}}(x), S_{h_{2}}\left(h_{1}(x)\right), \ldots, S_{h_{L}}\left(h_{L-1} \circ \cdots \circ h_{1}(x)\right)\right)$：多层网络的激活 pattern

$R_{\mathbf{h}}(s):=\left\{x \in \mathbb{R}^{n_{0}} | S_{\mathbf{h}}(x)=s\right\}$：多层网络的某一激活 pattern 对应的输入空间的区域。

$\mathcal{S}_{\mathbf{h}}:=\left\{S_{\mathbf{h}}(x) \in\{0,1\}^{n_{1}} \times \cdots \times\{0,1\}^{n_{L}} | x \in \mathbb{R}^{n_{0}}\right\}$：多层网络的所有可能的激活 pattern

我们所关心的就是 $|\mathcal{S}_{\mathbf{h}}|$ 的大小。激活 pattern 数量和区域划分数量的关系是：
$$
N_{f_{\mathrm{h}}} \leq\left|\mathcal{S}_{\mathbf{h}}\right|
$$
定义激活 pattern 序列：
$$
\begin{aligned}&\mathcal{S}_{\mathbf{h}}^{(1)}:=\left\{\left(s_{1}\right) \in\{0,1\}^{n_{1}} |\left(s_{1}, \ldots, s_{L}\right):=s \in \mathcal{S}_{\mathrm{h}}\right\}\\&\mathcal{S}_{\mathbf{h}}^{(2)}:=\left\{\left(s_{1}, s_{2}\right) \in\{0,1\}^{n_{1}} \times\{0,1\}^{n_{2}} |\left(s_{1}, \ldots, s_{L}\right):=s \in \mathcal{S}_{\mathbf{h}}\right\}\\&\vdots\\&\mathcal{S}_{\mathbf{h}}^{(L)}:=\quad\left\{\left(s_{1}, \ldots, s_{L}\right) \in\{0,1\}^{n_{1}} \times \cdots \times\{0,1\}^{n_{L}} |\left(s_{1}, \ldots, s_{L}\right):=s \in \mathcal{S}_{\mathrm{h}}\right\}=\mathcal{S}_{\mathrm{h}}\end{aligned}
$$
这个激活 pattern 序列的含义就是随着网络的一层一层的前向计算，网络输入的不同区域对应的激活 pattern 的集合。



**核心定义 histgram**：$\tilde{\mathcal{H}}^{(l)}\left(\mathcal{S}_{\mathrm{h}}^{(l)}\right)$ 为集合 $ \left\{\min \left(n_{0},\left|s_{1}\right|, \ldots,\left|s_{l}\right|\right)| s \in \mathcal{S}_{\mathbf{h}}^{(l)}\right\}$ 的 histgram，表示当前所有激活 pattern 的最小值 $\min\{n_0, |s_i|\}$ 的分布。$\min \left(n_{0},\left|s_{1}\right|, \ldots,\left|s_{l}\right|\right)$ 表示了某一激活 pattern $s$ 对应的区域经过 $\mathbf{h}$ 的运算得到的值域的最大可能维度，因此 $\tilde{\mathcal{H}}^{(l)}\left(\mathcal{S}_{\mathrm{h}}^{(l)}\right)$  表示了当前所有划分出来的区域经过 $\mathbf{h}$ 的运算后的值域的最大可能维度的分布。

严格的写法是：
$$
\mathcal{H}_{n^{\prime}}:\left\{\begin{array}{ll}
\mathcal{P}\left(\{0,1\}^{n^{\prime}}\right) & \rightarrow V \\
\mathcal{S} & \mapsto\left(\sum_{s \in \mathcal{S}} \mathbb{1}_{\{j\}}(|s|)\right)_{j \in \mathbb{N}}
\end{array}\right.\\
\tilde{\mathcal{H}}^{(l)}:\left\{\begin{array}{ll}
\mathcal{P}\left(\{0,1\}^{n_{1}} \times \cdots \times\{0,1\}^{n_{l}}\right) & \rightarrow V \\
U & \mapsto\left(\sum_{\left(s_{1}, \dots, s_{l}\right) \in U} \mathbb{1}_{\{j\}}\left(\min \left(n_{0},\left|s_{1}\right|, \ldots,\left|s_{l}\right|\right)\right)\right)_{j \in \mathbb{N}_{0}}
\end{array}\right.
$$
这两个定义则分别表示单层和多层网络的一个激活 pattern 集合对应的 histgram 的计算。

**对于 histgram，可以定义比较运算 $\preceq$，以及 $\max$ 运算。**

比较运算的定义：
$$
v \preceq w \quad: \Longleftrightarrow \forall J \in \mathbb{N}: \sum_{j=J}^{\infty} v_{j} \leq \sum_{j=J}^{\infty} w_{j}
$$
<img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200427113126073.png" alt="image-20200427113126073" style="zoom:50%;" />

$\max$ 运算的定义：
$$
\max _{i \in I}\left(v^{(i)}\right)_{J}=\max _{i \in I}\left(\sum_{j=J}^{\infty} v_{j}^{(i)}\right)-\max _{i \in I}\left(\sum_{j=J+1}^{\infty} v_{j}^{(i)}\right) \quad \text { for } J \in \mathbb{N}
$$
比较运算刻画了一个性质：总是希望划分以后的空间的维度尽可能大，这样进一步划分的数量会更多。

借助比较运算，我们的目的是得到下列的表达式：
$$
\begin{aligned}
\tilde{\mathcal{H}}^{(1)}\left(\mathcal{S}_{\mathbf{h}}^{(1)}\right) & \preceq \varphi_{n_{1}}^{(\gamma)}\left(\mathbf{e}_{n_{0}}\right) \\
\tilde{\mathcal{H}}^{(l+1)}\left(\mathcal{S}_{\mathbf{h}}^{(l+1)}\right) & \preceq \varphi_{n_{l+1}}^{(\gamma)}\left(\tilde{\mathcal{H}}^{(l)}\left(\mathcal{S}_{\mathbf{h}}^{(l)}\right)\right) \quad \text { for } l \in\{1, \ldots, L-1\}
\end{aligned}
$$
通过 histgram的 变换来确定最终的空间划分数量，而 $\varphi^{\gamma}_{n_i}$ 表示 histgram 的变化关系。

问题的核心就转化成  $\varphi^{\gamma}_{n_i}$，这里的 $\gamma$ 表示一个参数，不同的 $\gamma$ 决定了 $\varphi^{\gamma}_{n_i}$ 的好坏，决定了最终计算出来的区域划分数量的上界。



### 关于 $\varphi^{\gamma}_{n_i}$ 的详细计算说明

下面的定义给出了 $\gamma$ 的 具体内容：

1. $\forall n^{\prime} \in \mathbb{N}_{+}, n \in\left\{0, \ldots, n^{\prime}\right\} \quad \max \left\{\mathcal{H}_{n^{\prime}}\left(\mathcal{S}_{h}\right) | h \in \operatorname{RL}\left(n, n^{\prime}\right)\right\} \preceq \gamma_{n, n^{\prime}}$
2. $\forall n^{\prime} \in \mathbb{N}_{+}, n, \tilde{n} \in\left\{0, \ldots, n^{\prime}\right\} \quad n \leq \tilde{n} \Longrightarrow \gamma_{n, n^{\prime}} \preceq \gamma_{\tilde{n}, n^{\prime}}$

$n'\in \mathbb{N}^+,0\leq n\leq n'$

$\gamma_{n,n'}$ 表示当输入空间的维度是 $n$，划分的超平面数量是 $n'$ 时，对于输入空间的划分区域的值域的激活 pattern 的激活神经元数量的分布（是一个 histgram）的上界。

第一个条件的含义是 $\gamma_{n,n'}$ 的一个定义，它应该满足大于等于任何一个 $n$ 维输入空间、$n'$ 个超平面划分结果的值域的激活 pattern 的激活神经元数量的分布 histgram，因此也大于等于其输出 image 的维数 histgram。

第二个条件的含义是当输入空间的维数变大时，相同数量的超平面划分的上界 histgram 满足大小关系。

这两个条件刻画了递推式的本质含义。

为了方便，对于 $n = 0$ 的特殊定义：
$$
n^{\prime} \in \mathbb{N}_{+}, \operatorname{RL}\left(0, n^{\prime}\right)=\left\{h:\{0\} \rightarrow \mathbb{R}^{n^{\prime}}, x \mapsto c | c \in \mathbb{R}\right\}\\h \in \mathrm{RL}\left(0, n^{\prime}\right), \mathcal{H}_{n^{\prime}}\left(\mathcal{S}_{h}\right)=\mathrm{e}_{0}
$$
满足条件的 $\gamma_{n,n'}$ 的集合记为 $\Gamma$。对于 $n > n'$ 的情形，$\gamma_{n,n'}$ 取 $n = n'$  的情形即可。

注意：在这里 histgram 有两个具体含义，一个是 $\gamma_{n,n'}$ 中的激活 pattern 的激活神经元数量的 histgram，另一个是输出 image 的维度大小的 histgram。

考虑到输出 image 的维数小于等于输入空间的维数，因此即便激活 pattern 的激活神经元数量比 $n$ 大，最终的 image 的维度仍然是小于等于 $n$。所以为了建立 $\gamma$ （激活 pattern 的激活神经元数量的 histgram）到 $\tilde{\mathcal{H}}^{(l)}\left(\mathcal{S}_{\mathbf{h}}^{(l)}\right)$ （输出 image 的维度的 histgram），定义一个功能性函数 clipping function，作用在某一个 histgram 上：
$$
\mathrm{cl}_{i^{*}}(v)_{i}=\left\{\begin{array}{ll}v_{i} & \text { for } i<i^{*} \\ \sum_{j=i^{*}}^{\infty} v_{j} & \text { for } i=i^{*} \\ 0 & \text { for } i>i^{*}\end{array}\right.
$$
<img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200427113000777.png" alt="image-20200427113000777" style="zoom:50%;" />

则有：
$$
\left(\sum_{s \in \mathcal{S}_{h}} \mathbb{1}_{\{i\}}\left(\operatorname{rank} \text { of } h \text { on } R_{h}(s)\right)\right) \quad \preceq \mathrm{cl}_{n}\left(\gamma_{n, n^{\prime}}\right)
$$
因此可以定义：
$$
\varphi_{n^{\prime}}^{(\gamma)}:\left\{\begin{array}{ll}
V & \rightarrow V \\
v & \mapsto \sum_{n=0}^{\infty} v_{n} \mathrm{cl}_{\min \left(n, n^{\prime}\right)}\left(\gamma_{\min \left(n, n^{\prime}\right), n^{\prime}}\right)
\end{array}\right.
$$
利用 $v_1 \preceq v_2$ 可以推出 $\left\|v_{1}\right\|_{1} \leq\left\|v_{2}\right\|_{1}$，我们所关心的区域划分的上界 $\left|\mathcal{S}_{\mathbf{h}}\right|$ 可以表示为
$$
\left|\mathcal{S}_{\mathbf{h}}\right|=\left\|\tilde{\mathcal{H}}^{(L)}\left(\mathcal{S}_{\mathbf{h}}^{(L)}\right)\right\|_{1} \leq\left\|\varphi_{n_{L}}^{(\gamma)} \circ \cdots \circ \varphi_{n_{1}}^{(\gamma)}\left(\mathrm{e}_{n_{0}}\right)\right\|_{1} \quad \text { for } \gamma \in \Gamma
$$

### 最终形式

上述的计算恰好可以写成矩阵形式：
$$
\left(B_{n^{\prime}}^{(\gamma)}\right)_{i, j}=\left(\varphi_{n^{\prime}}^{(\gamma)}\left(\mathrm{e}_{j-1}\right)\right)_{i-1}=\left(\mathrm{cl}_{j-1}\left(\gamma_{j-1, n^{\prime}}\right)\right)_{i-1} \quad \text { for } i, j \in\left\{1, \ldots, n^{\prime}+1\right\}
$$
定义连接矩阵：
$$
M_{n, n^{\prime}} \in \mathbb{R}^{n^{\prime}+1 \times n+1}, \quad M_{i, j}=\delta_{i, \min \left(j, n^{\prime}+1\right)} \text { for } i \in\left\{1, \ldots, n^{\prime}+1\right\}, j \in\{1, \ldots, n+1\}
$$
例子：
$$
M_{4,2}=\left(\begin{array}{ccccc}1 & 0 & 0 & 0 & 0 \\0 & 1 & 0 & 0 & 0 \\0 & 0 & 1 & 1 & 1\end{array}\right), \quad M_{2,4}=\left(\begin{array}{ccc}1 & 0 & 0 \\0 & 1 & 0 \\0 & 0 & 1 \\0 & 0 & 0 \\0 & 0 & 0\end{array}\right)
$$
最终的计算为：
$$
\left|\mathcal{S}_{\mathbf{h}}\right| \leq\left\|B_{n_{L}}^{(\gamma)} M_{n_{L-1}, n_{L}} \ldots B_{n_{1}}^{(\gamma)} M_{n_{0}, n_{1}} e_{n_{0}+1}\right\|_{1}
$$
连接矩阵并不是必须的，只是为了写法上和 $B_{n'}^{(\gamma)}$ 相匹配。



## Bounding and Counting Linear Regions of Deep Neural Networks, Thiago Serra, Christian Tjandraatmadja, Srikumar Ramalingam, 2018, ICML 的上界在矩阵框架下的应用

任何一个上界，计算的核心是参数 $\gamma$，这里用 $\gamma_{n,n'}(i)$ 表示 $\gamma_{n,n'}$ 的第 $i$ 个分量（注意第一个分量是取值 0 的数量，为了记号上的方便，$i$ 从 0 开始） 。

在 2018 年的这篇文章中，有表达式：
$$
\left\{\begin{array}{ll}
0 & 0\leq i < n'-n\\
\left(\begin{array}{c}
n' \\
n'-i
\end{array}\right) & \max\{n'-n,0\} \leq i \leq n'
\end{array}\right.
$$
具体的例子 $\gamma_{\cdot,5}$

<img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200502113122818.png" alt="image-20200502113122818" style="zoom: 50%;" />

经过 clipping 函数的计算，可以得到 $B^{\gamma}_{5}$

<img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200502113149630.png" alt="image-20200502113149630" style="zoom:50%;" />



## 我们提出的上界计算

定义 downward-move 运算（histgram 到 histgram）：
$$
\text{dm}(v)_i = \left\{\begin{array}{ll}
0 & i = 0\\
v_{i-1} & i >0
\end{array}\right.
$$
这个运算可以看成是 histgram 的下标的移动和开头分量的补 0。

首先我们计算出 $n=1$ 时的，$\gamma_{1,n'}$ 的最小取值，换言之 $\max \left\{\mathcal{H}_{n^{\prime}}\left(\mathcal{S}_{h}\right) | h \in \operatorname{RL}\left(0, n^{\prime}\right)\right\} \preceq \gamma_{0, n^{\prime}}$ 的最紧的上界（因为存在某种划分，使得该上界可以达到），并给出严格的证明。

**命题 1** 在 1 维情形下，$\max \left\{\mathcal{H}_{n^{\prime}}\left(\mathcal{S}_{h}\right) | h \in \operatorname{RL}\left(0, n^{\prime}\right)\right\} = (0,\cdots,0, n'\mod{2},2,\cdots,2,1)^{T}$，$n'\mod{2}$ 的位置是 $n'- \lfloor (n')/2\rfloor$。最后一个 1 的位置是 $n'+1$。

证明略。

**命题 2** 在递推的初值条件满足 $\gamma$ 的定义的情况下，任何满足下式的 $\gamma_{n,n'},n'>1$ 都符合其定义：
$$
\gamma_{n,n'} = \gamma_{n-1,n'-1} + \text{dm}(\gamma_{n,n'-1})
$$
证明略。

由命题 1 和命题 2 可以给出$n> 1$ 时 $\gamma_{n,n'}$ 的所有取值，而 $n=1$ 由命题 1 得到，$n=0$ 时只要去 $\gamma_{0,n'} = (1,0,0,\cdots)$  即可。



## 和 Bounding and Counting Linear Regions of Deep Neural Networks, Thiago Serra, Christian Tjandraatmadja, Srikumar Ramalingam, 2018, ICML 的比较

事实上，可以证明 Bounding and Counting Linear Regions of Deep Neural Networks, Thiago Serra, Christian Tjandraatmadja, Srikumar Ramalingam, 2018, ICML 中的 $\gamma_{n,n'}$ 也满足命题 2，但是 $n=1$ 的取值不是严格的最紧的上界。因此我们提出的上界计算必然不比这篇文章提出的差。

比较：

![image-20200502153408951](/Users/xieyutong/Library/Application Support/typora-user-images/image-20200502153408951.png)

![image-20200502153449046](/Users/xieyutong/Library/Application Support/typora-user-images/image-20200502153449046.png)

同时也可以注意到，两者的差距并不时非常大（不知道当 $n'$ 很大的时候差别有多少）。

总结：我们给出的方法的核心是命题 2，这个命题在 Serra 2018 并没有出现，或者说他们证明上界的过程中，不是利用命题 2 来证明。而我们的上界的证明核心中，命题 2 是关键。如果 $n=1$ 的情形我们的取值和 Serra 2018 一样，那么最终的结果也是一样的。如果有更好的关于 $n$ 的结论，比如 $n = 2,n=3$ 这些低维情形的更紧的上界作为 $\gamma_{2,n'},\gamma_{3,n'}$，那么利用命题 2，就可以得到更紧的上界。



## 其他网络结构

### pooling

线性插值、average pooling、其他线性变换、strdie>0 的下采样，这四种等价于输入 $x$ 乘上矩阵 $A$，$A$ 一般都是满秩的，不妨设 rank(A) = k，则维度为 n 的 region，其 image 的维度为 $\min(k,n)$，所以当前划分区域的 image 的维度 histgram $v$ 的变化相当于 $\text{cl}_k(v)$，其矩阵形式和连接矩阵 $M$ 一样。

如果用的是 max pooling，这是一个非线性的结构，存在进一步的空间划分，根据 Serra 2018 的分析，一个 $k$-rank 的 $n_l$ 节点的 maxout 层（对于 stride=2 的 max pooling，假设输入是 $n$ 维，则 $n_l = n/4, k = 4$，记 $c = (k^2-k)n_l$，$v$ 的变化是：
$$
\text{cl}_{n_l}\left(diag\left\{\sum \gamma_{0,c},...,\gamma_{n,c}\right\} v\right)
$$


### unpooing

常见的线性插值、补0，这一类的运算仍等价于矩阵运算，且因为输出的维度大于输入的维度，所以 histgram $v$ 是不变的，只需要乘上相应的连接矩阵 $M$.

如果是 deconvolution，假设包含 ReLU 层，那么和一般的线性层 + ReLU 的变化是一样的。



### skip connection

这里考虑的 skip connection 是 concatenation 的形式。

有一个简单的引理，设 $x\in R$，$R$ 是一个 $k$ 维区域，则矩阵运算 $y = [I\text{ } A^T]^T x$，$R$ 映射之后 的区域的维度仍是 $k$.

假设 skip connection 定义在输入 $x$ 和输出 $y$ 之间，输入和输出之间的运算是某一网络结构，我们可以将这部分的 histgram 的变换记为：$v_y = A_t A_{t-1}\cdots A_1 v_x$，用矩阵计算得到一个综合矩阵，记为$B$，即 $v_y = B v_x$，因为 skip connection，实际的输出是 $(y\text{ } x)$，那么有结论：
$$
v_y = \sum v_{x,i}C_iBe_i
$$
$v_{x,i}$ 是 $v_x$ 的分量，$C_i$ 是一个第 $i$ 行全为 1，其余均为 0 的矩阵。上式可以简化为 $v_y = B'E v_x$ 的矩阵形式。$B' = [C_0B \cdots C_n B]$，$E = [E_{0,0}\cdots E_{n,n}]^T$

如果有多个 skip connection 的嵌套，不如 U-net，可以先计算内层的 skip connection，然后再计算外层的 skip connection。

dense connection 和 residual connnection 是 skip connection 的特例和推广。



## 一般的随机连接的网络

定义网络结构：首先根据网络的前向最长路径，将该网络的形式改写。每一层的节点分为两个部分，一部分是参与下一层节点的计算，一个部分参与之后的某一层中的计算，对于后者，就是跳跃连接。

示例：

<img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200502203027162.png" alt="image-20200502203027162" style="zoom:40%;" />

**命题 3** 设输入中参与运算的结点为 $x_1 \in \mathbb{R}^{n_{0,1}}$，不参与运算的结点为 $x_2 \in \mathbb{R}^{n_{0,2}}$，输出分为六个部分 $y_i \in \mathbb{R}^{n_{1,i}}$，分别表示 $x_1$ 运算后的输出中参与下一层运算的结点、$x_1$ 运算后的输出中会参与之后某一层的计算的结点、$x_2$ 中不参与下一层的计算而是之后某一层的计算结点、$x_2$ 中参与下一层的运算的结点（和上图的记号是一样的）。一般的随机连接的网络的单层形式可以表示为 $f(x_1, x_2) = (y_1, y_2, y_3, y_4)$，且 $(y_1, y_4)$ 相当于下一层的输入中的 $x_1$，$(y_2, y_3)$ 相当于下一层输入中的 $x_2$. 那么要计算的是划分区域的 image 的维度的分布 histgram 只要考虑 $x_1$ 部分的变化。即 $x_1$ 对应的 histgram 到 $(y_1, y_4)$ 的 histgram 的变化。不妨设 $x_1$ 对应的 region 的维度是 $n$，输入的 $v = e_{n}$，那么变换之后的 histgram 为：
$$
\text{cl}_{\min\{n,n_{1,1}\}+n_{1,4}}\left(\text{dm}^{n_{1,4}}\left(\gamma_{n,n_{1,1}} + \sum^{n_{1,2}-1}_{i=0}\text{cl}_{\min\{n, n_{1,1}\}}(\gamma_{n-1,n_{1,1}+i})\right)\right)
$$
上式构成了 histgram 变换的矩阵 $B$ 的第 $n+1$ 列。$B \in \mathbb{R^{n_{1,1}+n_{1,4}}}$，连接矩阵在这类网络的计算中也是需要的。

证明略。这个证明用到了命题 2 的证明的思路。