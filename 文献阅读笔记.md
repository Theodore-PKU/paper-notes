Deep convolutional framelets a general deep learning framework for inverse problems, 2018

hankel matrix
$$
\mathbb{H}_{d}(f)=\left[\begin{array}{cccc}
f[1] & f[2] & \cdots & f[d] \\
f[2] & f[3] & \cdots & f[d+1] \\
\vdots & \vdots & \ddots & \vdots \\
f[n] & f[1] & \cdots & f[d-1]
\end{array}\right]
$$
SISO:
$$
y=f(*) \bar{\psi}=\mathbb{H}_{d}(f) \psi
$$
MIMO:
$$
\mathbb{H}_{d | p}(Z):=\left[\mathbb{H}_{d}\left(z_{1}\right) \quad \mathbb{H}_{d}\left(z_{2}\right) \quad \cdots \quad \mathbb{H}_{d}\left(z_{p}\right)\right]\\
$$

$$
\begin{aligned}
Y &=Z \otimes \bar{\Psi} \\
&=\sum_{j=1}^{p} \mathbb{H}_{d}\left(z_{j}\right) \Psi_{j} \\
&=\mathbb{H}_{d | p}(Z) \Psi
\end{aligned}
$$

这篇文章利用 hankel matrix 给出了一个信号 $f$ 完美重建的条件：
$$
\begin{aligned}
\tilde{\Phi} \Phi^{\top} &=\sum_{i=1}^{m} \tilde{\phi}_{i} \phi_{i}^{\top}=I_{n \times n} \\
\Psi \tilde{\Psi}^{\top} &=\sum_{j=1}^{q} \psi_{j} \tilde{\psi}_{j}^{\top}=I_{p d \times p d}
\end{aligned}
$$
此时，local basis $\mathbf{\Psi}$ 和 non-local basis $\mathbf{\Phi}$ 构成了 encoder-decoder 的结构：
$$
\begin{aligned}
&Z=(\tilde{\Phi} C) \otimes \nu(\tilde{\Psi})\\
&C=\Phi^{\top}(Z \otimes \bar{\Psi})
\end{aligned}
$$
其中，
$$
\nu(\tilde{\Psi}):=\frac{1}{d}\left[\begin{array}{ccc}
\tilde{\psi}_{1}^{1} & \cdots & \tilde{\psi}_{1}^{p} \\
\vdots & \ddots & \vdots \\
\tilde{\psi}_{q}^{1} & \cdots & \tilde{\psi}_{q}^{p}
\end{array}\right] \in \mathbb{R}^{d q \times p}
$$
接下来作者讨论了 bias 的情形，证明 encoder 的 bias 和 decoder 的 bias 在满足一定条件下，原来的 perfect reconstruction 结果仍然成立。
$$
b_{d e c}[i]=-1_{d}^{\top} \tilde{\Psi}_{i} b_{e n c}, \quad i=1, \cdots, p
$$
推广到 multi-layer 的情形：
$$
\begin{aligned}
f &=\mathbb{H}_{d_{(1)} | p_{(1)}}^{\dagger}\left(\tilde{\Phi}^{(1)} \hat{C}^{(1)} \tilde{\Psi}^{(1) \top}\right) \\
&=\left(\tilde{\Phi}^{(1)} \hat{C}^{(1)}\right) \otimes \nu\left(\tilde{\Psi}^{(1)}\right)
\end{aligned}
$$

$$
\hat{C}^{(i)}=\left\{\begin{array}{ll}
\mathbb{H}_{d(i+1) | p_{(i+1)}^{\dagger}}\left(\tilde{\Phi}^{(i+1)} \hat{C}^{(i+1)} \tilde{\Psi}^{(i+1)}\right)=\left(\tilde{\Phi}^{(i+1)} \hat{C}^{(i+1)}\right) \otimes \nu\left(\tilde{\Psi}^{(i+1)}\right), & 1 \leq i<L \\
C^{(L)}, & i=L
\end{array}\right.
$$

$$
C^{(i)}=\left\{\begin{array}{ll}
\Phi^{(i) \top} \mathbb{H}_{d_{(i)} | p_{(i)}}\left(C^{(i-1)}\right) \Psi^{(i)}=\Phi^{(i) \top}\left(C^{(i-1)} \otimes \bar{\Psi}^{(i)}\right), & 1 \leq i \leq L \\
f, & i=0
\end{array}\right.
$$

接下来作者考虑引入 ReLU 函数。利用两倍的 filter，实现 perfect reconstruction：
$$
\Psi^{(l)}=\left[\Psi_{+}^{(l)} \quad-\Psi_{+}^{(l)}\right], \quad \tilde{\Psi}^{(l)}=\left[\begin{array}{ll}
\tilde{\Psi}_{+}^{(l)} & -\tilde{\Psi}_{+}^{(l)}
\end{array}\right]\\
\begin{equation}
\Psi_{+}^{(l)} \tilde{\Psi}_{+}^{(l) \top}=I_{p_{(l)} d_{(l)} \times p_{(l)} d_{(l)}}, \quad \Psi_{+}^{(l)}, \tilde{\Psi}_{+}^{(l)} \in \mathbb{R}^{p_{(l)} d_{(l)} \times m_{(l)}}
\end{equation}
$$
然后讨论了 filter 数量不满足 perfect reconstruction 的情形，指出对于信号 $X$，只要 filter 的数量大于 $2*rank(\mathbb{H}_{d|p}(X))$，当 $\mathbb{H}_{d|p}(X)$ 的秩很小的时候，仍然存在 local basis，使得 perfect reconstruction 成立。

接下来证明了 ResNet 的结构下，存在 local basis，使得 perfect reconstruction 成立。

讨论说明 fewer filter 具有 shrinkage 的效果，即限制重建的信号是低秩的 。

讨论了 bypass connection (实际上是 residual learning 的效果)。

根据 perfect reconstruction 的条件，指出 U-net 结构的不足之处在于 non-local basis 不满足使 perfect reconstruction 的条件成立，而 skip connection 也无法弥补 pooling 的信息损失。

综上的一些讨论和理论分析，在神经网络的改进上，主要的内容是 non-local basis 的设计，结合小波变换的形式设计和改进了 pooling 和 unpooling 的结构。

![image-20200408145806745](/Users/xieyutong/Library/Application Support/typora-user-images/image-20200408145806745.png)

我的理解：主要的想法是基于信号的分解和合成（类似于 framelet），但是拓展到 multi-layer 的结构。encoder-decoder 框架的合理在于符合这种信号分解和合成的表示，这对应 perfect reconstructon 的理论 ，hankel matrix 的低秩特点是该结构在图像相关问题上有效的根本原因，这对应 fewer filters 的解释。

Problems:

1. 关于 bias 的讨论，意味着假设 bias 不存在也没有关系，换言之，bias 不是网络所必须的结构，反而在没有 bias 的情况下，更容易满足 perfect reconstruction。

   疑问：在这样的理论下，没有 bias 也可以保证 perfect reconstruction，那么 BN 层和卷积层的 bias 都可以去掉？

2. 关于 ReLU 函数，为了保证 perfect reconstruction，ReLU 函数产生了负面的 效果，为了弥补这一点，需要将 filter 的数量翻倍，并且 filter 需要满足相反的关系。

   疑问：在训练网络时，并没有对 filter 有相反关系的约束，其次实验中是否有相反关系的 filter 存在？

3. skip connection 的解释除了 U-net 结构中有意义，在 dense-net 的结构中也是有意义的，但是 deep convolution framelet 中，skip connection 不太好解释（特别是 dense-net 不具有对称性）。
4. perfect reconstruction 成立的条件中，要求 local basis 满足一定的条件，但实际的神经网络训练中，并没有关于这个条件的约束。
5. 作为逆问题重建的解决手段，实际中，bias、ReLU 是两个重要的组成部分，但是这两个部分对于理论核心 perfect reconstruction 没有正面的意义。
6. 无法完全解释网络深度对于重建的意义，对于 perfect reconstruction 和 shrinkage 两个核心而言，网络深度并不直接相关。
7. 难以分析 data-driven 的训练方式，训练样本对于 local basis 的优化是如何作用的？少量的训练样本和大量的训练样本的差别。
8. 稳定性分析的疑问：如果仅仅是信号的 hankel matrix 低秩就可以实现 perfect reconstruction，那么是否存在对抗攻击的样本？



Understanding Geometry of Encoder-Decoder CNNs, 2019

这篇文章从以下几个方面对 encoder-decoder 结构的网络进行分析：

- Differential Topology
- Links to the frame representation
- Expressiveness
- Generalizability
- Optimization landscape

1. Differential Topology 主要是引用了 Shen, H. A differential topological view of challenges in learning with feedforward neural networks 的两个结论（但这部分的讨论我不太明白意义是什么）

2. Links to the frame representation 论述了 deep convolutional framelet 理论的相关结论。

3. Expressiveness 部分对 frame representation 提出了一些批判，"the perfect reconstruction condition itself is not interesting in neural networks, since the output of the network should be different from the input due to the task dependent processing", 也就是说 deep convolutional framelet 理论中的核心概念 perfect reconstruction 在网络输入和输出不同的情况下，没有讨论的必要。在 Expressiveness 小节中，作者考虑了 ReLU 情形下的 input region partition。对 with/without skip connection 两种 encoder-decoder 计算 input region partition 的数量上限。给出的结论是：
   $$
   N_{r e p}=2^{\sum_{i=1}^{\kappa} d_{i}-d_{\kappa}}\\
   N_{r e p}=2^{\sum_{i=1}^{\kappa} d_{i}-d_{\kappa}} \times 2^{\sum_{i=1}^{\kappa} s_{k}}
   $$
   （但是我认为这个结论是不对的）。作者通过分割的数量来衡量网络的表达能力。如果这个结论是正确的，反而无法解释网络的深浅的区别，因为对于 浅层网络，只要 filter 的数量变多，在总的参数量相近的情况下可以达到和深层网络相同的分割数量上界。

4. Generalizability 讨论的是网络的 Lipschitz constant 的性质。基于  input region partition 数量的有限性，指出 "local Lipschitz constant within each linear region, one could control the global Lipschitz constant of the neural network".
5. Optimization landscape 部分所讨论的内容是从 cost function 的 Jacobian 矩阵的性质出发，说明 encoder-decoder network 的 optimization landscape 的性质（这部分我也不太明白 optimization landscape 的含义是什么？研究的目的是 什么？



Geometric Approaches to Increase the Expressivity of Deep Neural Networks for MR Reconstruction, 2020

这篇文章可以看成是作者在 Understanding Geometry of Encoder-Decoder CNNs 的基础上，关于 MRI 重建的网络改进。可以分成两个部分，一是理论分析，而是网络设计。

理论分析 

1.  这篇文章中，作者仍然是用网络对 input 的分割数量 $2^{\# \text{ of neurons}}$ 来刻画网络的表达能力，但也指出实际上因为 ReLU pattern 不是完全独立的，不可能达到这个上界。

2. 关于 encoder 的分析，以对 input 的分割为主，讨论的是 Geometric Meaning of Features。核心的分析是：
   $$
   \begin{aligned}
   \boldsymbol{z}^{l-1} &=\sigma\left(\boldsymbol{E}^{(l-1) \top} \boldsymbol{z}^{l-2}\right) \\
   &=\boldsymbol{\Lambda}\left(\boldsymbol{z}^{l-1}\right) \boldsymbol{E}^{(l-1) \top} \boldsymbol{z}^{l-2}
   \end{aligned}
   $$
   "This implies that the hyperplane in the current layer is adaptively changed with respect to the input data"，我觉得文章中这样的表述并不合适，但整体的分析保持一致。

3. 关于 decoder 的分析，就不再考虑 input 的分割，而是 Geometric Meaning of Decoder，认为 "the synthesis frame $D^l Λ({\tilde{ξ}} ^l )$ is indeed a subset of the original synthesis frame."，把 ReLU 的不同输入下的 pattern 当作是 synthesis frame 的变形。
4. 关于 skip connection，分析中考虑的不是 concatenation，而是 addtion 的情形，认为是增加了冗余性。

> 关于理论分析，decoder 的分析不如 encoder 的分析严密，我认为不够严谨的地方在于 decoder 在形式上和 encoder 是一样的，在 encoder 的讨论中考虑了对 input 的分割，但是在 decoder 中就不考虑 input 的分割了，而从 synthesis frame 的自适应性角度分析，这样做显得不够合理。另外冗余性的含义是什么，相比划分的区域数量而言，没有一个严格的定义

在这样的分析之下，研究的目标变成要增加网络的表达能力和冗余性。一个简单的做法是使用多个网络，然后用 attention layer 把各个网络的结果合并起来。接下来考虑用什么样的网络结构可以达到同样的效果，但是增加非常少的网络参数量。

网络设计

基于以上的想法，作者设计了三种方式改进 MRI 重建的网络。

1. Bootstrap subsampling. 即通过不同的 kspace 系数作为输入（也就是在 undersample 的基础上进行 subsample）

   <img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200409150614441.png" alt="image-20200409150614441" style="zoom: 50%;" />

   attention 网络是 MLP

<img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200409150732390.png" alt="image-20200409150732390" style="zoom:50%;" />

2. Adaptive Residual Learning. 即 $\boldsymbol{v}=w_{1}(\boldsymbol{z}) \boldsymbol{z}+w_{2}(\boldsymbol{z}) \mathcal{T}_{\Theta}(\boldsymbol{z})$

<img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200409150842162.png" alt="image-20200409150842162" style="zoom:50%;" />

​	attention layer 的结构是 1x1 卷积

3. Iterative Aggregation. 即合并迭代网络的多个中间值结果，attention layer 的结构是 1x1 卷积。

   <img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200409151037196.png" alt="image-20200409151037196" style="zoom:50%;" />



从这三篇文章的演进看，似乎从 deep convolutional framelet 转向了 encoder-decoder 结构的几何分析。前者指导下的改进在于 pooling 层，后者的改进在于增加 expressiveness 和 redundancy。



### 输入区域划分数量的表达式

这部分想了很久，如果只考虑一层的线性层和 ReLU 的组合，这个表达式相当于：对 $n$ 维空间，用 $k$ 个超平面进行划分，最多可以分出多少个区域。这个问题的确有解（自己想了很久，后来发现这个问题已经有答案了），设 $f(n,k)$ 表示该问题的解，根据递推表达式 
$$
f(n, k) = f(n-1, k-1) + f(n, k-1)
$$
结合组合数公式 $\left(\begin{array}{l}n \\ i\end{array}\right)=\left(\begin{array}{c}n-1 \\ i\end{array}\right)+\left(\begin{array}{c}n-1 \\ i-1\end{array}\right)$，可以求出
$$
f(n, k) = \sum^n_{i=0} \left(\begin{array}{l} k \\ i\end{array}\right)
$$
但是这只解决了一层线性层和 ReLU 的组合的情形，分析多层的情况，如果我们假设初始输入划分的每个子区域在当前层的输出空间中的表示都可以被下一层的线性层和 ReLU 组合最大化的继续划分，用数学语言表达就是设初始输入 $x_0\in\mathbb{R}^{d_0}$，经过 $l\in\mathbb{R}^{d_l}$ 层后，输出是 $x_l$，对于初始输入而言，产生的划分效果是 $\{x_0^i|i = 1, 2, ..., n_l\}$，有 $n_l$ 个划分区域，每个区域因为 ReLU 层导致某些维度上聚合成一个点，因此在 $\mathbb{R}^{d_l}$ 中并不都是 $d_l$ 维的区域（这里的意思是说需要用多少个线性不相关的基向量表示该区域上的所有点），假设这些区域的维数是 $d^i_l$，也就是说 $x_0^i$ 这个区域经过 $l$ 层后的输出 $x_l^i \in \mathbb{R}^{d_l^i}$，经过第 $l+1$ 层的运算（$k_{l+1}$ 个超平面）后，$x^i_l$ 都被 $k_{l+1}$ 个超平面在 $\mathbb{R}^{d_l^i}$ 意义上最大化划分，那么 $l+1$ 层运算的最终划分效果就是 $\bigcup^{n_l}_{i=1}\{x^i_l 的划分在 x^i_0 上的对应划分\} $. 因此划分数量应当是
$$
n_{l+1} = \sum^{n_l}_{i=1} f(d_l^i, k_{l+1})
$$
因此需要知道 $d_l^i$ 的具体取值。但是 $f(n, k) = \sum^n_{i=0} \left(\begin{array}{l} k \\ i\end{array}\right)$ 这个式子无法解决该问题，我们要得到的应该是 $f(n,k,d)$，表示 $n$ 维空间被 $k$ 个超平面划分之后，属于 $d$ 维区域的数量 $0\leq d \leq n$，满足 $\sum_{d=0}^n f(n,k, d) =f(n,k)$. 举一个两层和三层网络的例子，最终的分割数量应该是：
$$
n_2 = \sum^{k_1}_{d=0} f(d_0,k_1,d)f(d,k_2)\\n_3 = \sum^{k_1}_{d_1=0}\left(f(d_0,k_1,d_1)\sum^{d_1}_{d_2=0} f(d_1,k_2,d_2)f(d_2, k_3)\right)
$$
目前这个问题还没有准确的解。首先前述的假设 **“$x^i_l$ 都被 $k_{l+1}$ 个超平面在 $\mathbb{R}^{d_l^i}$ 意义上最大化划分”** 还无法保证是否一定成立，换句话说，我们计算的其实是上界，这里的核心计算是 $f(n,k,d)$，目前没找到准确的解，但是有一个上界和下界的递推表达式，同时可以设计一个近似平均概念的期望值递推表达式。这个非准确的结果我会尝试一下看能不能有比较具体的计算表达式。

上界：
$$
f(n,k+1,d) = f(n, k, d-1) + f(n-1, k,d)
$$
下界：
$$
f(n,k+1,d) = f(n,k,d) + f(n-1,k,d-1)
$$
平均期望（即上界和下界的平均值）：
$$
f(n,k+1,d) = \dfrac{1}{2}\left(f\left(n, k,d-1\right) + f\left(n-1, k,d\right) + f\left(n, k,d\right) + f\left(n-1, k,d-1\right)\right)
$$
1维情形下区域划分的维数分布

| 维数 | 总数 | 0    | 1    | 2    | 3    | 4    | 5    | 6    |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 1    | 2    | 1    | 1    |      |      |      |      |      |
| 2    | 3    | 0    | 2    | 1    |      |      |      |      |
| 3    | 4    | 0    | 1    | 2    | 1    |      |      |      |
| 4    | 5    | 0    | 0    | 2    | 2    | 1    |      |      |
| 5    | 6    | 0    | 0    | 1    | 2    | 2    | 1    |      |
| 6    | 7    | 0    | 0    | 0    | 2    | 2    | 2    | 1    |

2维情形下区域划分的维数分布

| 维数      | 总数 | 0      | 1     | 2     | 3     | 4      | 5     | 6    |
| --------- | ---- | ------ | ----- | ----- | ----- | ------ | ----- | ---- |
| 1         | 2    | 1      | 1     |       |       |        |       |      |
| 2（精确） | 4    | 1      | 2     | 1     |       |        |       |      |
| 2（上界） |      | 1      | 2     | 1     |       |        |       |      |
| 2（下界） |      | 1      | 2     | 1     |       |        |       |      |
| 2（平均   |      | 1      | 2     | 1     |       |        |       |      |
| 3（精确） | 7    | 0      | 3     | 3     | 1     |        |       |      |
| 3（上界） |      | 0      | 3     | 3     | 1     |        |       |      |
| 3（下界） |      | 1      | 2     | 3     | 1     |        |       |      |
| 3（平均） |      | 0.5    | 2.5   | 3     | 1     |        |       |      |
| 4（精确） | 11   | 0      | 2     | 4     | 4     | 1      |       |      |
| 4（上界） |      | 0      | 1     | 5     | 4     | 1      |       |      |
| 4（下界） |      | 1      | 2     | 4     | 3     | 1      |       |      |
| 4（平均） |      | 0.25   | 2     | 4.25  | 3.5   | 1      |       |      |
| 5（精确） | 16   | 0      | 0     | 5     | 5     | 5      | 1     |      |
| 5（上界） |      | 0      | 0     | 3     | 7     | 5      | 1     |      |
| 5（下界） |      | 1      | 2     | 4     | 5     | 3      | 1     |      |
| 5（平均） |      | 0.125  | 1.125 | 4.125 | 5.875 | 3.75   | 1     |      |
| 6（精确） | 22   | 0      | 0     | 3     | 6     | 6      | 6     | 1    |
| 6（上界） |      | 0      | 0     | 1     | 5     | 7      | 6     | 1    |
| 6（下界） |      | 1      | 2     | 4     | 6     | 5      | 3     | 1    |
| 6（平均） |      | 0.0625 | 0.625 | 3.125 | 6.5   | 6.8125 | 3.875 | 1    |

三维情形下的维数划分

| 维数      | 总数 | 0    | 1    | 2    | 3    | 4    | 5    | 6    |
| --------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 1         | 2    | 1    | 1    |      |      |      |      |      |
| 2         | 4    | 1    | 2    | 1    |      |      |      |      |
| 3         | 8    | 1    | 3    | 3    | 1    |      |      |      |
| 4（精确） | 15   | 0    | 4    | 6    | 4    | 1    |      |      |
| 4（上界） |      |      |      |      |      |      |      |      |
| 4（下界） |      |      |      |      |      |      |      |      |
| 4（平均） |      |      |      |      |      |      |      |      |







That is, even if the bottom layers have random initializations, it is likely that the network should work well enough subject to a learned mapping at the top layer. 这个观点有点神奇。 data is less important, rather it is the topology of the DL network that is able to capture the essence of the data



信息瓶颈理论：**Tishby 和 Shwartz-Ziv还提出了一个有趣的发现，即深度学习分为两个阶段：一个简短的“拟合”阶段，在此期间，网络学着去标注其训练数据，以及一个更长时间的“压缩”阶段，在这个阶段网络变得能够泛化，也即标记新的测试数据。**









逼近理论很难，我们的目的是找出

深度学习机制大牛：普林斯顿，明尼苏达应用数学研究所，

哈佛 MIT UCLA

深度学习是大坑，不要陷进去。最后要回归到实际问题中

