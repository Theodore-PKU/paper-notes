## 深度学习机制的思考

神经网络的基本组件是线性层和非线性层。给定输入，得到输出。输出的内容取决于任务，最简单的是分类问题，因此先考虑分类问题。

从最简单的情形入手，线性层是全连接层，非线性层是 ReLU。



**对二维散点进行分类**

假设这些点是线性可分的

<img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200403202451418.png" alt="image-20200403202451418" style="zoom: 33%;" />

存在一个超平面，$w^T x + b = 0$ 将这些点分开（箭头表示超平面的方向，位于指向侧的点，$w^T x + b < 0$，另一侧相反）。这时候不需要 ReLU 激活函数，线性层就是最终分类器。初始的坐标 $x = (x_1, x_2)$ 本身就是性质良好的特征。



**如何理解深度学习的机制？**

需要解释的问题有

1. 非线性层（激活函数）的作用是什么？
2. 浅层结构和深层结构的区别是什么（深层结构的作用是什么）？



先不考虑网络训练的问题，直接假定我们可以做到网络参数的全空间搜索。



**对于第一个问题：非线性层的作用是什么？**

假设一个复杂一点的二分类问题。

<img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200403204258613.png" alt="image-20200403204258613" style="zoom:33%;" />

对于圈点⭕️，对于给定的数据样本，最少用四个超平面可以将其和叉点❌分开，假定这四个超平面的方向如图所示，那么神经网络的第一层的结构就是：

<img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200403204825493.png" alt="image-20200403204825493" style="zoom:33%;" />

经过 ReLU 层后，对于圈点⭕️，可以表示为 $(0,0,0,0)$，对于叉点❌，可以表示为 $(a_1,a_2,a_3,a_4), a_i$ 不全为 $0$ 且 $\geq 0$。此时，任意一个 $w = (w_1, w_2, w_3, w_4), w_i > 0$， $b$ 取一个绝对值非常小的负数，必然可以将特征空间上的圈点⭕️和叉点❌线性可分。



在这个二分类的例子，依靠超平面对圈点⭕️边界的刻画，以及圈点⭕️本身是聚成一团的简单的拓扑性质，可以将圈点⭕️用四个超平面（这四个超平面构成了一个特征提取）和 ReLU 函数映射到 $(0,0,0,0)$。所以可以将原始的二维空间的线性不可分的情形变成线性可分的情形。只要圈点⭕️的拓扑性质很简单（直觉上说就是连通的，可以被包括在有限个超平面构成的区域中），这种方式都可以将原来线性不可分的数据分布变换成线性可分。

如果圈点⭕️不是连通的，比如可以分成有限个团，那么同样可以用多个超平面将这些“团”各自划分出来。经过 ReLU 变换之后，似乎不能直接变成线性可分的情形。但是可以再次通过超平面的第二次划分来改变当前的线性不可分情形。考虑如下的一个数据分布（左图）。用 $x_1 = 0, x_2 = 0$ 两个超平面可以对四个区域进行划分，经过 ReLU 变换之后，可以得到（右图）

![image-20200404102706940](/Users/xieyutong/Library/Application Support/typora-user-images/image-20200404102706940.png)

在有限个数据样本点情况下，可以找到两个非常接近于数轴的超平面，把所有的叉点❌划分在一个区域中，再经过 ReLU 运算，就可以实现线性划分。



总结：线性层和 ReLU 层从直观上可以实现的效果是通过区域的划分，不同区域因为 ReLU 运算，改变了原始的空间分布，使其简单化，变得更易划分，或者说，出现新的空间分布形态。



**对于第二个问题：深层结构和浅层结构的区别是什么？**

可以证明三层网络可以拟合任意函数（仅仅考虑二次可积这种符合一般要求的函数）。

**证明**：只要证明三层网络可以表达出任一逼近函数。将函数定义域（网络输入）用超平面划分为 $n$ 个区域，第 $i$ 个区域用 $m_i$ 个超平面包住，且这 $m_i$ 个超平面的方向选取为使得该区域的所有点对于这 $m_i$ 个超平面的线性运算取值为负，那么对于第 $i$ 个区域之外的所有点，关于这 $m_i$ 个超平面的线性运算必然不全为负数。经过 ReLU 计算之后，按照这个 $n$ 个区域各自对应的 $m_i$ 个超平面排列，顺序是
$$
s_1, s_2, ..., s_{m_1}, s_{m_1+1}, ..., s_{m_1+m_2}, ...,s_{\sum^{n-1}_{k=1}m_k +1}, ...,s_{\sum^{n}_{k=1}m_k}
$$
只有第 $i$ 个区域的点（先不考虑边界）在 $s_{\sum^{i-1}_{k=1}m_k +1},...s_{\sum^{i}_{k=1}m_k}$ 上取值全为0，其他区域的点必有取值为正数的情况。第一层的输出是 $\sum^{n}_{k=1}m_k$ 维。

第二层的线性层可以设计为 $n$ 个线性运算（对应 $n$ 个节点输出），也就是 $n$ 个超平面，第 $i$ 个线性层对应第 $i$ 个区域，其权重取值是：在 $s_{\sum^{i-1}_{k=1}m_k +1},...s_{\sum^{i}_{k=1}m_k}$ 上取负数，bias 取一个极小的正数 $\delta_i$，再经过 ReLU 变换，那么对于第 $i$ 个区域上不靠近区域边界的点，在第二层的第 $i$ 个节点的输出就是 $\delta_i$，而在其他节点的输出全为 $0$，靠近区域边界上的点，在第二层的第 $i$ 个节点的输出仍是 $\delta_i$，但在相邻的区域（假设是第 $j$ 个区域），因为 bias $\delta_j$ 是一个极小的正数，因此即便权重都是负数，只要无穷接近边界，就仍可能是非零值。但是只要 $\delta_i$ 取足够小，就可以让第 $i$ 个区域上几乎所有的点在第二层的第 $i$ 个节点上的输出为 $\delta_i$，在其他节点输出为 $0$。比如下图的示例，虚线区域（虚线取决于 $\delta_j$）的点的第二层输出为 $(0,0, ..., \delta_i,0,0,...0)$. 对于其他区域也是一样的结果。

<img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200404102949793.png" alt="image-20200404102949793" style="zoom:33%;" />

假设这 $n$ 区域的取值分别是拟合的目标函数在这些区域上的均值 $f(i)$（一个粗糙的假定，属于黎曼积分的考虑范畴），即第三层线性层定义为 $(\dfrac{f(1)}{\delta_1}, ...,\dfrac{f(i)}{\delta_i},... \dfrac{f(n)}{\delta_n})$，那么对于每个区域中的虚线包含的部分，经过第三层的线性层运算，其结果就是 $f(i)$，也就是所在区域的均值。

和目标函数的误差来源有两项，一项是均值替代该区域的所有函数取值，一项是第一层超平面划分区域的边界附近的区域（即图中不属于虚线内部的但仍属于区域 $i$ 的）的取值不准确。但是当 $n$ 趋于无穷时，第一项误差会趋于 0，在给定第一项误差时，$\delta_i$ 取无穷小，可以让第二项误差趋于 0，因此总的误差可以趋于 0. 

因此三层网络（线性层 + ReLU）可以无穷逼急任意函数。**证毕。**

注：该命题适用于分类问题。



类似的证明过程，如果我们将第三层线性层的输出从 $1$ 变成 $m$，那么就可以拟合多值函数。这种情况适用于图像生成、分割这一类问题。



但是从上述分析可以看出，完美逼急需要的线性层的数量是非常大，特别是第一个线性层，要实现完美的空间划分，需要非常多超平面。由此提出一个猜想，深层网络和浅层网络的差别在于对空间的划分效率，即深层网络，在相同参数量的情况下，可以划分出比浅层网络更多的子区域。

没想出怎么证明，有一个思路，但是无法写出计算的一般表达式。思路：假设多层网络各层的节点数为 $n_1, n_2, n_3, ..., n_k$，我们需要直接计算出可能的最大空间划分数量，然后再用数学推导证明深层网络的划分效率。但是第一步不知道怎么计算显式的结果。

不过，可以考虑一个简单情形：比较 $n,n,n$ 和 $n, 2n$ 的差别。前者是一个两层网络，第一个 $n$ 表示输入的维数。后者表示一个单层网络，仅仅比较空间划分的数量（超平面的参数量相同）。

假设在 $k$ 维空间，使用 $t$ 个超平面可以实现的空间划分数量为 $f(k, t)$，有一些简单结论，$f(k, t) = O(t^k/k!)$， $f(k,t)=2^t, t\leq k$，$f(k,t) = f(k,t-1) + f(k-1, t-1)$. 理论上应该可以得到 $f(k,t)$ 的表达式，因为递推式和初值都是已知的。（但是没推出来.....）

考虑 $n-n-n,n-2n$ 的情形，前者在第一层的划分中，得到 $2^n$ 个子空间，在第二层的进一步划分中，并不是 $2^n\cdot2^n$，因为有些区域局限在维度更低的子空间中，其分布满足组合数 $C^i_n,i = 0, ..., n$. $i$ 为该区域对应的子空间的维度。所以应当是 $\sum^n_{i=0}C^i_n f(i,n)$ 是最终的空间划分数量的上限。由于不知道 $f(i,n)$ 的表达式，考虑一个特例 $n = 4$，通过手动计算，可以得到空间划分的数量是 163，后者的空间划分数量是 $f(4, 8) = 130$. 因此多层网络的空间划分效率高于浅层网络。



总结：深层网络和浅层网络的区别在于空间划分的效率，目前只有一个直觉上和经验上的理解，缺乏严格的证明。



**拓展**

基于以上思考，考虑以下两个问题。

拓展问题 1：ReLU 和其他激活函数的区别？

拓展问题 2：如何解释不同的 connection（concatenation），residual 结构？

对于第一个问题，首先激活函数的特性是单调和非线性。在要解决的任务中，神经网络表示的映射一般是单射，单调保证不会将超平面划分的不同区域映射到相同的点，同时一定程度上保持相同区域内的距离顺序不变。非线性的作用是改变空间分布，对于 ReLU 函数而言是一个非常极端而简单的例子，将超平面某一侧的点都映射为 0.

对于第二个问题，connection 相当于扩大了当前层的维数，并且用此前的数据的某一形态的分布状态来帮助当前空间的划分。很重要的优点可以从 U-net 的 skip connection 来理解，越初始的空间划分，因为经过 ReLU 或其它激活函数的次数比较少，不容易出现本应该划分开的点被分到一个区域（比如因为 ReLU 映射为 0），而经过数层的变换，这种情况有可能发生，这样就很难将应该分开的点划分开，利用先前的信息（因为在浅层还没有合到一起）就可以进行划分了。这也就是所谓的解决信息消失的一种方式。信息融合也很好理解，增加了其他可能用于空间划分的维度，或者说通过增加维度使其存在超平面进行划分。

第二个问题中，residual 结构比较难理解（global 的 residual 结构不属于这个问题，只有局部 feature 的 residual 结构是相关的），还没想清楚。



**BN 层**

关于 BN 的理解，其本身是一个平移 + 放缩的线性变化，不影响空间划分。BN 层的作用，应该体现在优化方面。



**生成模型**

空间划分 + 超定方程？



**下周的想法**

能不能设计一些验证实验。