[link](file:///Users/xieyutong/Documents/Research/Paper Reading/Notes/什么是深度学习——刘利刚.md)

### 什么是深度学习——刘利刚

#### 函数逼近论

在函数逼近论中，最终的函数表示是基函数的线性和。

函数逼近的两个核心概念：1. 基函数的选择；2. 基函数的个数（表达能力，参数量）。同时这也是两个无法直接解决的问题。

稀疏学习、稀疏优化、特征选择的本质是基函数的选择。字典学习相当于稀疏优化和基函数学习同时进行。

参数曲线曲面的拟合，通常的做法是先进行参数化，然后用参数化表示原始数据。而且，往往希望参数区域的网格满足原始数据分布中的几何特性。

一元函数的逼近形式的神经网络表达：

<img src="/Users/xieyutong/Pictures/screenshot/image-20200415112422925.png" alt="image-20200415112422925" style="zoom:50%;" />

多元多值函数的逼近形式的神经网络表达：

<img src="/Users/xieyutong/Pictures/screenshot/image-20200415112526463.png" alt="image-20200415112526463" style="zoom:45%;" />

这里只有一层隐层的原因是这个隐层的每个神经元表示了一个不同的基函数 $\varphi_n$.

参数曲线曲面的拟合过程的神经网络表示和 AE 结构非常类似：

<img src="/Users/xieyutong/Pictures/screenshot/image-20200415113022434.png" alt="image-20200415113022434" style="zoom:40%;" />

#### 深度学习可以理解成自动学习基函数

类似于小波变换中基函数的表示，给定一元函数 $y = \varphi(x)$，称之为元函数，则 $y = \varphi^a_b(x) = \varphi(ax + b)$ 和元函数线性无关，该线性组合具有很强的表达能力。这对于多变量的情形同样成立，只不过 $ax+b$ 变成多元的仿射变换。神经网络中的激活函数就相当于元函数，线性层就相当于仿射变换，这样就构成了一个神经元的结构。对于多元的输出，只不过是由多个共享的基函数构造的。

但函数逼近论中的问题仍然存在，使用什么基函数（元函数）？使用多少个基函数（网络的宽度）？

万能逼近定理给出了这种元函数拓展的函数集合作为基函数的表达能力的理论保证。

但是刚才的讨论只有三层网络，实际中使用的都是深层网络。深层网络可以看成是复合函数，同样有逼近任何函数的理论证明。

#### 进一步理解深度学习

ReLU 激活函数的输出是分片线性函数。激活函数必须是非线性的，否则无法拟合非线性函数。

每个隐层看成是对输入做参数化，多个隐层就是多次参数化。

曲面拟合中也存在类似的做法，每次参数化都相对简单，这样可以构造出复杂的参数化。

![image-20200415114901459](/Users/xieyutong/Pictures/screenshot/image-20200415114901459.png)

ResNet 是学习映射的一阶差分，DenseNet 是学习映射的高阶差分。