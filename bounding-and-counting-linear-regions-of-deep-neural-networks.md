### Bounding and Counting Linear Regions of Deep Neural Networks

Author: Thiago Serra, Christian Tjandraatmadja, Srikumar Ramalingam

Year: 2018

Published in: ICML 2018

```latex
@article{serra2017bounding,
  title={Bounding and counting linear regions of deep neural networks},
  author={Serra, Thiago and Tjandraatmadja, Christian and Ramalingam, Srikumar},
  journal={arXiv preprint arXiv:1711.02114},
  year={2017}
}
```

Link: https://github.com/Theodore-PKU/paper-notes/blob/master/bounding-and-counting-linear-regions-of-deep-neural-networks.md

Note: /Users/xieyutong/Documents/Research/PaperReading/Notes/bounding-and-counting-linear-regions-of-deep-neural-networks.md



#### 简介

分析的思路是：对于属于分片线性函数的神经网络，其表达能力的复杂度的一种刻画是分片的数量。

前人已经有不少相关的工作

1. Pascanu, R., Montúfar, G., and Bengio, Y. On the number of response regions of deep feedforward networks with piecewise linear activations. In ICLR, 2014.
2. Bianchini, M. and Scarselli, F. On the complexity of neural network classiﬁers: A comparison between shallow and deep architectures. IEEE Transactions on Neural Networks and Learning Systems, 2014.
3. Montúfar, G., Pascanu, R., Cho, K., and Bengio, Y. On the number of linear regions of deep neural networks. In NIPS, 2014
4. Raghu, M., Poole, B., Kleinberg, J., Ganguli, S., and SohlDickstein, J. On the expressive power of deep neural networks. In ICML, 2017.
5. Montúfar, G. Notes on the number of linear regions of deep neural networks. In SampTA, 2017.



这篇文章的贡献：

1. 更紧的上界和下界
2. 对于宽度小于输入维度的网络，浅层的网络表达能力更强
3. 一个具体的计算网络的分割数量的方法

#### 记号和背景

这篇文章考虑的是 ReLU DNN，最后一层是线性层。非线性层数量为 ，每一层的输入的维度是 ，输出维度是 ，最后的线性层输出维度是 



作者定义的分片数量是激活层的 pattern 数量。设  表示第  层的神经元被激活的位置， 就定义了一个激活 pattern。只要对某一输入 ，其激活 pattern 是 ，那么它对应相应的一个线性变换，因此这些  构成了分片的区域。分片区域的数量就是分片数量。

另一种定义中，不考虑激活 pattern，而是只考虑对应相同线性变换的区域。事实上，不同的激活 pattern，可能对应相同的线性变换，因此这种定义中的分片数量比作者定义的分片数量少。

**一个基本事实**： 维空间被  个超平面划分的区域最大数量是 .

#### 更紧的界

##### 上界

作者给出的上界是（定理1）：给定记号和背景中的 ReLU DNN，分片数量的最大值的上界是



其中，，当  时，这是紧的上界。

> 证明：
>
> 一些记号说明
>
> -  表示第  层激活 pattern 对应的  的矩阵（部分行变成 0）
> - 
> -   是输入的一个区域，对一个单独的一个激活 pattern，则第  层的输出的空间维度是 
>
> 三个重要引理：
>
> **lemma 4**: 如果超平面是由  定义的，那么区域划分的数量是 
>
> **lemma 5**: 给定输入区域 ，经过  层，分片数量最多是 
>
> **lemma 6**: 设  是经过  层后的某个激活 pattern 的原始输入空间的一个分片区域， 是经过  层的某个激活 pattern 的原始输入空间的一个分片区域，且 包含 ，也就是说激活 pattern 在前  层是一样的。那么 
>
> 这三个引理都是对的，而且描述非常准确。引理 4 是区域划分的更精确的表述。引理 5 描述了分片数量和上一层输出维度的关系，引理 6 描述了相邻层输出维度的关系。
>
> 具体证明过程：
>
> 设  表示对于某一个区域 ，其在  层的输入是  维，经过  层后，对  的划分数量。有递推式
>
> 
>
> 核心是第二个式子。 是因为  维空间映射到  维空间时，如果 ，其结果仍然是  维（这一点以前没注意到）。 表示  维输入被  个超平面划分后，激活 pattern 中激活数量  的区域的数量。
>
> 作者利用该式，给出了如下的放缩结果：
>
> 
>
> 第一部分可以这样理解：激活 pattern 中激活数量为  的区域数量一定不超过 ，因为相当于从  个超平面中选择  个，这些区域的输出 image 的维度是 。而  等于事实上可以划分的最多区域数量，而  如果 ，那么从最大的  开始，也就是  开始，每次取的数量超过实际可能取到的数量，且最后加和等于 ，可以看成是：
>
> 
>
> 也就是：
>
> 
>
> 这在  的条件下是必然成立的。利用 ，所以得到作者给出的递推式子。利用递推式可以得到最后的结论（化简形式了）
>
> 证毕。
>
> **我们需要仔细讨论的就是  的估计，很显然，作者这里放缩得很大，不过也得益于此，可以有简洁的表达式**

引理5和引理6的一个核心观点是每次分割出来的区域，其维度不会增加，只可能变少。这和下面的推论直接相关。

从这个上界可以得到两个推论（证明都在附录A）：

1. bottleneck 效果：当网络输出的维度比中间层的宽度大时，浅层位置的神经元数量比深层位置的神经元数量对分片数量的影响更大。作者给出了一个**命题**（命题2）：考虑一个两层网络，如果 ，则从第一层移动一个神经元到第二层，分片数量的上界严格递减。不过这个命题只针对上界，并不是真实的最大值。不过从实验的结果来看，确实满足这个结论。
2. Deep vs shallow 当输入维度很大的时：这个推论和上一个推论很像。具体说来，当输入维度特别大时，浅层网络的表示能力比深层网络更强。**推论**（推论3）：设 ，则 ，且 

##### 上界的特例：

 时，分片数量的准确上界是 。证明在附录D。

> 证明思路：基本想法是构造一个网络，使得其分片数量的准确上界为结论中所述。假设输入是一维上的 ，输出是  个神经元，对于一元的输入，超平面就是线段上的分割点，用  个点分割 ，一共有  个分割区域，每个分割区域对应输出 image 的维度不是 0 就是 1，取决于是否存在某个神经元被激活了。作者设计的分割方式（其实就是分割点的左侧激活还是右侧激活），第3个点选择左侧激活，其余点选择右侧激活。这样构造的结果就是可以实现对  划分出  个区域，因为每个区域的 image 都是线段。主要的问题在于我们要考虑多层，而不是单层。证明的核心在于对于  层的  个神经元的输出 ，作者构造了（实际上可能前人的文章也用了类似的方法）一个线性组合  使得  层对  划分后的任何一个区域，经过该线性组合的运算（记为 ）后又变成  线段，可能是 0 到 1，也可能是 1 到 0. 换言之， 在  上是一个 Z 字型的折线，而  取值为 1 或 -1.
>
> <img src="/Users/xieyutong/Pictures/screenshot/image-20200420165754718-2048731.png" alt="image-20200420165754718" style="zoom:33%;" />
>
> 每个区域映射的结果都是 。我们可以这样构造网络，第一层输入是一维的 ，输出是  维，第二层的运算分成两步，第一步先用一个  运算（这个运算具体和  的值有关，所以每一层都不一样），将  维向量变成一元的 ，根据前面的构造可知，对于第一层输入的任何一个分割区域上的点，其  值都属于 ，且每个子区域刚好映射为 . 第二步是对  进行  次分割，分割方式仍然是第3个点选择左侧激活，其余点选择右侧激活。这样以来两步的复合构成了从  到  的线性变换和 ReLU 运算， 对应的就是  个神经元到第二层第  个神经元运算（注意，这里的  是一元的线性变换，和  的复合才是多元的线性变换，不过  的作用就和第一层的神经元的线性运算一样）。其结果就是第一层实现的分割子区域经过第二层会再次被分割，且分割数量就是 ，那么两层的分割就是 。类似地，只要后续的每一层都实现相同的步骤，那么每次最新的细分区域都会因为  运算重新变成 ，这样就可以很轻松地继续划分了。其划分数量的结果就是命题的结论。
>
> 最后就是计算出具体的线性运算 。根据定义（忽略第几层）， 要满足图中的条件，设分割点是 ，那么可以列出  个方程，使得  个区域可以映射成 :
>
> 
>
> 这个方程组中， 和  合并了，实际上因为第三个点选择的激活方向是左侧，所以  应该是负数。其他的点选择的激活方向是右侧，所以  应该是正数。上述方程组的前四个可以解出  之间的关系（两个方程），这四个数存在无穷多解，使得 。另外  都可以用  表示，且符号符合条件（即  是分割点）。因此每一层  都可以列出相应的方程组，并且求出解。
>
> 证毕。
>
> **在这个证明中，构造的关键是第三个点的激活方向和其他不同，因此命题的条件中要求每一层的神经元数量大于等于 3. 类似这样的证明过程，如果只有第二个点激活方向和其他不同，方程组的求解中会发现矛盾，所以无解。倘若只有第二个点激活方向不同也是成立的，那么  的情形也适用。然而，实际上，如果 ，仅仅考虑第一层的输出（一个二维图），会发现不存在两条直线可以同时将三个线段各自分割出三个区域。**
>
> **从以上的证明和讨论结果来看，确实在  的情况下，作者提出的上界就是准确的上界，而对于  的情形并不是，因为我们已经画出一个两层网络无法实现 9 个区域的划分了。其中的关键在于  这个运算本身就不成立，因为每个子区域的 image 不一定可以同时划分出  个子区域。所以我们讨论的也肯定是上界，而不是准确的上界。**

##### 下界

作者的思路和 Montúfar et al. (2014) 类似，都是取构造一个网络。作者考虑的情形是 ，给出的下界是：



定理 9 给出了另外一个网络的构建，给出了一个下界，和 Arora 的工作相关，此处不赘述。这些内容并不是本篇文章的重点。

#### Maxout 网络的上界

在这个部分，作者考虑了激活函数是 maxout：



上述式子的意思是每一层会计算  个输出，然后再取最大值，这里的最大值应该 element-wise 决定的（相当于从  中选择某一行），被称为  maxout layer。这里的激活 pattern 应该就可以看成是某个神经元取的是相同的  吧，因为这样对应的复合线性矩阵是相同的。

maxout 类型和分片线性函数的关系是很大的，因为分片线性函数可以写成  这种形式。

作者给出的定理是：设网络是  层，每一层都是  节点输出，且 maxout layer 都是 rank-k，则最大的区域划分数量是：



> 证明：首先很容易发现，maxout 的运算对应一个复合的线性矩阵，这个矩阵的每一行的来源取决于 max 操作（对应一个激活 pattern ），设这个复合矩阵是 ，和空间划分有关的则是当前层的 maxout 操作，每一行有  个选择，在  维空间中，这  个选择就一个  维超平面，其中的  维是输入 ，剩下一维是 ，maxout 操作对应的空间划分就是这些超平面的交，这些交位于  维空间，在输入空间上的投影就变成输入空间  维空间的超平面，这些超平面的划分对应区域的划分。因为超平面的交的数量是 ，有  行，每行的超平面的交是独立的，所以总共有  个超平面，也就是  个超平面在  维空间上的划分。这个划分和  中的激活 pattern 的 是意一一对应的。假设  这个激活 pattern 对应的区域是 ，那么第  层的效果相当于在   上分割，而分割的超平面是 ， 分别表示 k 个矩阵中的两个， 表示第  个神经元输出。因此分割的运算就是由  构成的一个矩阵，和前面的  层复合矩阵相乘后，得到 
>
> 
>
> 对  的分割就是由  决定。很容易分析出 ，根据引理 4，得到对  的细分数量是 ，由于下次细分就是在  的子区域中进行，和其他子区域是独立的，所以可以得到上界是一个连乘形式
>
> 
>
> 证毕。

总结两个上界的证明：第一个 RelU 的上界证明中，放缩来源有两个，一个是每个子区域的相同激活数量 image 的数量 ，一个是递推式中每个子区域在下一层的划分中一定可以被完美划分。第二个 maxout 的上界证明中，放缩来源有一个，就是连乘形式暗含的每个子区域在下一层的分割中也都是可以被完美划分。事实上，下一层划分基本上无法实现对每个子区域都完美划分，因为很有可能，存在某些超平面，在这些子区域的 image 的外面。这些子区域的 image 在当前输出的空间中也是相连的，有可能出现超平面无法同时分割所有的 image。

#### 其他

关于划分区域的 counting 部分先暂时不看，还有其他的一些命题的证明也先不看。