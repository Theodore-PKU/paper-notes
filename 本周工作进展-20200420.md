# 本周工作进展

[TOC]

## 上周讨论内容

1. 写文章的时候要把想法来源的脉络写清楚，可以从16年UCLA的文章开始，再到2019年Jong Chul Ye的文章。
2. 假设给定网络的结构，存在一个计算该网络表达能力（划分区域）的算法。那么应该可以有一个算法，在给定网络架构和参数量的情况下，计算具体多少层、每层多少 filter 的网络结构可以使得网络表达能力最强。
3. 在2的基础上，再进一步，如何度量训练数据的复杂度，假设可以度量训练数据的复杂度，就可以找到对应个的参数量，那么在2的算法的辅助下，给定网络架构，就可以自动根据训练数据获得一个最优的网络结构。或者说自适应地调节参数量，并自适应地增加网络深度或宽度。使得整个网络设计自动化。
4. 理解深度学习的角度有很多，信息论、动态系统、空间划分、近似论。特别是近似论，找找看相关论文，思考如何与空间划分结合起来。



## 本周目标

- [ ] 1. 思考常见的网络结构的区域划分数量的计算，比如 skip connection（包括 concatenation 和 plus 两种）、residual 结构、dense connection 结构等，甚至多分支网络、多尺度卷积层。

- [ ] 2. 设计算法，在给定网络具体结构的情况下，计算区域划分的数量，用上界和下界以及平均结果的递推式。

- [ ] 3. 思考在不同网络结构，是否有一个统一的计算区域划分数量的方法。

- [ ] 4. 思考在给定参数量的情况下，是否有一个合适的优化算法，给出最优的网络结构设计。

- [ ] 5. 思考训练数据复杂度的度量方式，假设4的算法存在，如何自适应调节网络结构。

- [ ] 6.  整理上周未汇报的内容，思考并且设计实验，如果可以跑实验。

- [ ] 7. 整理深度学习理论的相关论文，分类，找近似论相关的论文。

- [ ] 8. 把不同思路的论文都稍微看一下，主要看近似论相关的。

- [ ] 9. 重新看三篇综述文章，研究语法、用词。

- [ ] 10. 在原来报告的基础上，重新调整，逐字逐句修改，并且把 PET 部分也写上。

- [ ] 11. 有时间开始动笔写英文综述。



## 4.14

### 论文列表

#### 表达能力

逼近论相关：

1. Approximating Continuous Functions by ReLU Nets of Minimal Width, 2017, 函数拟合，网络宽度和深度

2. Error bounds for approximations with deep ReLU networks, 2017,  函数拟合，网络深度

3. Universality of deep convolutional neural networks, 2020, 逼近论

4. Understanding deep neural networks with rectified linear units, 2018,  函数表示，表达能力

   

网络深度和宽度讨论：

1. Beneﬁts of depth in neural networks, 2016,  网络深度

2. On the Depth of Deep Neural Networks: A Theoretical View, 2016,  网络深度，表达能力

3. On the Expressive Power of Deep Learning: A Tensor Analysis, 2016,  网络深度，表达能力
4. Exponential expressivity in deep neural networks through transient chaos, 2016, 网络深度，表达能力
5. The Power of Depth for Feedforward Neural Networks, 2016, 网络深度
6. On the Expressive Power of Deep Neural Networks, 2017, 网络表达能力
7. Optimization Landscape and Expressivity of Deep CNNs,  2018,  网络表达能力
8. The power of deeper networks for expressing natural functions, 2018, 网络表达能力
9. On Exact Computation with an Infinitely Wide Neural Net, 2019, kernel method, 网络宽度



网络其他问题：

1. Understanding intermediate layers using linear classiﬁer probes, 2016, 网络深度，中间层理解

2. A Closer Look at Memorization in Deep Networks,  2017, 记忆性，数据集
3. Understanding Batch Normalization, 2018, BN 层
4. Do deep convolutional nets really need to be deep and convolutional?, 2017, 卷积和深度
5. Geometric deep learning: going beyond Euclidean data, 2017, 深度学习应用于非欧数据
6. A State-of-the-Art Survey on Deep Learning Theory and Architectures, 2019, review



其他流派解释：

1. Opening the black box of Deep Neural Networks via Information,  2017, 信息瓶颈解释深度学习
2. Understanding Autoencoders with Information Theoretic Concepts, 2019, 信息论
3. On the Information Bottleneck theory of Deep Learning, 2018, 信息瓶颈理论
4. Deep Learning Theory Review: An Optimal Control and Dynamical Systems Perspective, 2019, 动态系统
5. A Geometric Understanding of Deep Learning,  最优传输理论
6. A Differential Topological View of Challenges in Learning with Feedforward Neural Networks, 2018, 网络表达能力、泛化、优化理论
7. Why does deep and cheap learning work so well?, 2017, 网络结构、表达能力、物理解释
8. Nonlinear random matrix theory for deep learning, 2019, 随机矩阵理论
9. Spectrally-normalized margin bounds for neural networks, 2017, margin theory
10. on the Margin Theory of Feedforward Neural, 2019, margin theory, kernel method
11. Neural Tangent Kernel: Convergence and Generalization in Neural Networks, 2018, kernel method 观点
12. To Understand Deep Learning We Need to Understand Kernel Learning, 2018, 神经网络和 kernel 方法的关联



#### 泛化理论

正则化：

1. In search of the real inductive bias: On the role of implicit regularization in deep learning, 2015, 网络的正则性质
2. Implicit Regularization in Deep Learning, 2017, 博士论文，深度学习中的正则化
3. Implicit Regularization in Matrix Factorization, 2017, 作者同上，深度学习中的正则化
4. Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning, 2018, 正则性质



泛化：

1. Exploring Generalization in Deep Learning, 2017, 深度学习泛化能力的来源
2. Generalization in Deep Learning, 2017,  泛化误差理论
3. Understanding deep learning requires rethinking generalization, 2017, 泛化理论
4. Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks, 2019, 泛化理论
5. DNN or k-NN: That is the Generalize vs. Memorize Question, 2018, 泛化和记忆性，和其他分类模型的比较
6. Stronger generalization bounds for deep nets via a compression approach,  2018, 泛化误差理论



优化与泛化：

1. Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes, 2017, 损失函数和泛化
2. Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks, 2019, 优化和泛化



GAN：

1. Generalization and Equilibrium in Generative Adversarial Nets (GANs), 2017, GAN 生成的分布的泛化性及理论

2. Do GANs Learn the Distribution? Some Theory and Empirics, 2018, GAN 理论



#### 优化性质

1. On the Computational Efﬁciency of Training Neural Networks, 2014, 网络训练，优化
2. The Loss Surfaces of Multilayer Networks, 2015, 优化
3. Deep learning without poor local minima, 2016, 网络优化
4. Train faster, generalize better: Stability of stochastic gradient descent, 2016, SGD 优化
5. Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical Viewpoints, 2017, SGLD 的优化性质，泛化误差
6. SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data, 2018, SGD 避免过拟合，优化
7. On the Optimization Landscape of Tensor Decompositions, 2017, 优化 
8. On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization, 2018, 网络深度和优化
9. On the Power of Over-parametrization in Neural Networks with Quadratic Activation,  2018, 超参数化和优化
10. Implicit Regularization in Deep Matrix Factorization, 2019, 优化和正则化
11. Gradient descent provably optimizes over-parameterized neural networks, 2019, 优化理论



### 综述文章阅读

##### 用词、短语搭配

feasible, outstanding, researchers, report improvements, with a focus on, point to some next steps in the ﬁeld, give a very condensed history, speciﬁcally, spurred a resurgence, summarize the recent works, due to,  various, inaccurate, Anatomical priors, additional, investigated, deserves

##### 语法

过去分词做状语： Motivated by these successes, researchers have begun to apply CNNs to the resolution of inverse problems.

定语从句：These approaches, which used networks with a few parameters and did not always include learning, were largely superseded by compressed sensing approach.



## 4.15

### 什么是深度学习——刘利刚

#### 函数逼近论

在函数逼近论中，最终的函数表示是基函数的线性和。

函数逼近的两个核心概念：1. 基函数的选择；2. 基函数的个数（表达能力，参数量）。同时这也是两个无法直接解决的问题。

稀疏学习、稀疏优化、特征选择的本质是基函数的选择。字典学习相当于稀疏优化和基函数学习同时进行。

参数曲线曲面的拟合，通常的做法是先进行参数化，然后用参数化表示原始数据。而且，往往希望参数区域的网格满足原始数据分布中的几何特性。

一元函数的逼近形式的神经网络表达：

<img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200415112422925.png" alt="image-20200415112422925" style="zoom:50%;" />

多元多值函数的逼近形式的神经网络表达：

<img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200415112526463.png" alt="image-20200415112526463" style="zoom:45%;" />

这里只有一层隐层的原因是这个隐层的每个神经元表示了一个不同的基函数 $\varphi_n$.

参数曲线曲面的拟合过程的神经网络表示和 AE 结构非常类似：

<img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200415113022434.png" alt="image-20200415113022434" style="zoom:40%;" />

#### 深度学习可以理解成自动学习基函数

类似于小波变换中基函数的表示，给定一元函数 $y = \varphi(x)$，称之为元函数，则 $y = \varphi^a_b(x) = \varphi(ax + b)$ 和元函数线性无关，该线性组合具有很强的表达能力。这对于多变量的情形同样成立，只不过 $ax+b$ 变成多元的仿射变换。神经网络中的激活函数就相当于元函数，线性层就相当于仿射变换，这样就构成了一个神经元的结构。对于多元的输出，只不过是由多个共享的基函数构造的。

但函数逼近论中的问题仍然存在，使用什么基函数（元函数）？使用多少个基函数（网络的宽度）？

万能逼近定理给出了这种元函数拓展的函数集合作为基函数的表达能力的理论保证。

但是刚才的讨论只有三层网络，实际中使用的都是深层网络。深层网络可以看成是复合函数，同样有逼近任何函数的理论证明。

#### 进一步理解深度学习

ReLU 激活函数的输出是分片线性函数。激活函数必须是非线性的，否则无法拟合非线性函数。

每个隐层看成是对输入做参数化，多个隐层就是多次参数化。

曲面拟合中也存在类似的做法，每次参数化都相对简单，这样可以构造出复杂的参数化。

![image-20200415114901459](/Users/xieyutong/Library/Application Support/typora-user-images/image-20200415114901459.png)

ResNet 是学习映射的一阶差分，DenseNet 是学习映射的高阶差分。



## 4.16

- [x] 整理昨天阅读的四篇文献
- [ ] 下载新的和逼近论有关的文献
- [ ] 继续阅读这些文献

### Approximating Continuous Function by ReLU Nets of Minimal Width, Boris Hanin and Mark Sellke, 2017

这篇文章考虑的是 $w(d_{in}, d_{out})$，它的定义是当  ReLU 网络在逼近任意连续函数 $f:[0,1]^{d_{in}}\rightarrow \mathbb{R}^{d_{out}}$时，对任意的逼近精度，总是存在一个网络，宽度满足最宽处为 $w$，$w(d_{in},d_{out})$ 就是对于该函数，满足该条件的 $w$ 的最小值。

整篇文章是关于 $w(d_{in}, d_{out})$ 的上下界。结论是：
$$
d_{i n}+1 \leq w_{\min }\left(d_{i n}, d_{o u t}\right) \leq d_{i n}+d_{o u t}
$$
换句话说，当网络宽度为 $d_{in} + d_{out}$ 时，对任意逼近精度，都存在一个网络满足逼近条件，但是网络深度取决于函数本身和逼近精度；当网络宽度 $\leq d_{in}$ 时，总是存在某一连续函数，使得该网络和该函数之间的逼近程度有限 （无法无限逼近）。

证明思路：分成上界和下界两个部分。

上界部分：可以构造一个网络，宽度为 $d_{in}+d_{out}$ 时，可以实现以任意精度逼近任何一个函数。

下界部分：构造一个函数，使得任何一个宽度为 $d_{in}$ 的网络，都无法毕竟逼近该函数。

上界部分的证明中，作者先证明如下的函数可以等价于一个 ReLU 网络（$\ell$ 是线性变换）
$$
g=\sigma_{L-1}\left(\ell_{L}, \sigma_{L-2}\left(\ell_{L-1}, \ldots, \sigma_{2}\left(\ell_{3}, \sigma_{1}\left(\ell_{1}, \ell_{2}\right)\right) \cdots\right)\right.
$$
等价的 ReLU 网络具有比较特别的结构：
$$
A_{j}(x, y)=\left\{\begin{array}{ll}
\left(x, y-\ell_{j}(x)\right), & \text { if } \sigma_{j-1}=\max \\
\left(x,-y+\ell_{j}(x)\right), & \text { if } \sigma_{j-1}=\min
\end{array}\right.
\\
A_{j}^{-1}(x, y)=\left\{\begin{array}{ll}
\left(x, y+\ell_{j}(x)\right), & \text { if } \sigma_{j-1}=\max \\
\left(x,-y+\ell_{j}(x)\right), & \text { if } \sigma_{j-1}=\min
\end{array}\right.\\
H_{j}:=A_{j}^{-1} \circ \operatorname{ReLU} \circ A_{j}\\
$$
网络结构可以表示为：
$$
\mathrm{ReLU} \circ H_{L} \circ \cdots \circ H_{1}, H_1 = A_1 = (x, l_1(x))
$$
然后证明 $g$ 可以任意逼近任意函数 $f:\mathbb{R}^{d_{in}}\rightarrow \mathbb{R}^{d_{out}}$. $g$ 的证明则是通过从较小的定义域（半径很小的球）出发，不断增加定义域的范围（球面处增加一小部分，实际是一个三角区域，多次累加可以覆盖整个球面，使得定义域球的半径变大），同时增加 $g$ 的层数，使得在新的定义域上可以逼近函数 $f$。$g$ 增加的内容形如
$$
\widehat{g}=\max \left(\ell_{-}, \min \left(\ell_{+}, g\right)\right)
$$
 而 $\ell_{-}, \ell_+$ 这两个线性变换和增加的区域是直接相关的（和三角区域的顶点相关）。换言之，逼近函数 $g$ 的构造和区域划分是直接相关的，这种构造方法可以理解为某种形式的区域划分，然后逐个区域给出对应的函数值。

下界部分的证明中，作者先给出宽度为 $d_{in}$ 时表示的函数的一个性质，然后利用这个性质用反证法证明存在一个函数，其水平集函数是无法逼近的。



### Error bounds for approximations with deep ReLU networks, Dmitry Yarotsky, 2017

这篇文章的结论分为两个部分，一个是 ReLU 网络逼近函数时的上界，即存在 ReLU 网络，给定逼近精度时，其参数量、网络深度有限并且可以实现这一精度的逼近；另一个是下界，即 ReLU 网络在给定逼近精度时，至少需要多少结点或计算量，否则无法实现该精度的逼近。

首先作者讨论了不同分片线性激活函数和 ReLU 激活函数的不同网络之间的相互转换关系。

上界部分的证明思路

先证明 $x^2, xy$ 可以被逼近，具体为：

1. 对任意精度 $\epsilon$，存在 ReLU 网络，深度和节点数是 $O(\ln (1/\epsilon))$，可以逼近 $[0,1]$ 上的 $f(x) = x^2$

   逼近方式是通过函数 $g$ 的复合：
   $$
   g(x)=\left\{\begin{array}{ll}
   2 x, & x<\frac{1}{2} \\
   2(1-x), & x \geq \frac{1}{2}
   \end{array}\right.
   $$

   $$
   g_{s}(x)=\underbrace{g \circ g \circ \cdots \circ g}_{s}(x)\\
   f_{m}\left(\frac{k}{2^{m}}\right)=\left(\frac{k}{2^{m}}\right)^{2}, \quad k=0, \ldots, 2^{m}\\
   f_{m-1}(x)-f_{m}(x)=\frac{g_{m}(x)}{2^{2 m}}
   $$

   $f_m$ 是对 $f$ 的逼近，而 $f_{m}(x)=x-\sum_{s=1}^{m} \frac{g_{s}(x)}{2^{2 s}}$，因此，可以设计如下的网络结构：

   <img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200417192909251.png" alt="image-20200417192909251" style="zoom:50%;" />

2. 利用 $x y=\frac{1}{2}\left((x+y)^{2}-x^{2}-y^{2}\right)$，可以构建出在 $[0, M]\times[0,M]$ 上满足 $\epsilon$ 精度的 $xy$ 逼近网络，其深度和计算节点不多于 $c_{1} \ln (1 / \epsilon)+c_{2}, c_2 = c_2(M)$

然后证明对任意函数的逼近方式。对任意函数 $f$ 用泰勒展开的形式逼近，具体写为：
$$
f_{1}=\sum_{\mathbf{m} \in\{0, \ldots, N\}^{d}} \phi_{\mathbf{m}} P_{\mathbf{m}}
$$
$\boldsymbol{m}$ 表示格点，$\phi_m$ 类似于冲击函数。

<img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200417193615072.png" alt="image-20200417193615072" style="zoom:50%;" />
$$
P_{\mathbf{m}}(\mathbf{x})=\left.\sum_{\mathbf{n}:|\mathbf{n}|<n} \frac{D^{\mathbf{n}} f}{\mathbf{n} !}\right|_{\mathbf{x}=\frac{\mathbf{m}}{N}}\left(\mathbf{x}-\frac{\mathbf{m}}{N}\right)^{\mathbf{n}}
$$
很显然，$P_m(x)$ 是局部泰勒展开。则 $f_1$ 可以写为：
$$
f_{1}(\mathbf{x})=\sum_{\mathbf{m} \in\{0, \ldots, N\}^{d}} \sum_{\mathbf{n}: \mathbf{n} |<n} a_{\mathbf{m}, \mathbf{n}} \phi_{\mathbf{m}}(\mathbf{x})\left(\mathbf{x}-\frac{\mathbf{m}}{N}\right)^{\mathbf{n}}
$$
每一项 $\phi_{\mathbf{m}}(\mathbf{x})\left(\mathbf{x}-\frac{\mathbf{m}}{N}\right)^{\mathbf{n}}$ 都可以利用 $xy$ 的逼近设计 ReLU 网络。最终的逼近网络的深度最多为 $c(\ln (1 / \epsilon)+1)$，计算结点数为 $c \epsilon^{-d / n}(\ln (1 / \epsilon)+1)$，$c = c(d,n)$。$d$ 指的是 $f$ 的自变量的维数，$n$ 是和导数性质有关的量。$d, n$ 表示了一类函数：
$$
F_{n, d}=\left\{f \in \mathcal{W}^{n, \infty}\left([0,1]^{d}\right):\|f\|_{\mathcal{W}^{n, \infty}\left([0,1]^{d}\right)} \leq 1\right\}
$$


下界部分的结论则是给定不同条件下，至少需要多深、多少结点的 ReLU 网络的假设才能逼近目标函数。



### Universality of Deep Convolutional Neural Networks, Ding-Xuan Zhou, 2018

这篇文章考虑的是卷积网络的表达能力，给出了当深度满足一定条件时，CNN 可以逼近任何一个连续函数。其证明的核心，非卷积神经网络中的线性计算可以用多个卷积层替换。因此只要利用非卷积神经网络的逼近能力，就能证明CNN 的逼近能力。



### Understanding Deep Neural Networks with Rectfied Linear Units, Raman Arora, Amitabh Basu, Poorya Mianjy, Anirbit Mukherjee, 2018

这篇文章首先讨论了 ReLU DNN 和分片线性函数之间的关系，指出任何一个 ReLU DNN 都表示了分片线性函数，每个分片线性函数 $f:\mathbb{R}^{n} \rightarrow \mathbb{R}$ 都可以用深度最多为 $\left\lceil\log _{2}(n+1)\right\rceil+1$ 的 ReLU DNN 表示。

这部分的证明的要点是先把分片线性函数表示为：
$$
f=\sum_{j=1}^{p} s_{j}\left(\max _{i \in S_{j}} \ell_{i}\right)
$$
$\ell_i$ 都是仿射变换，也就是和每个分片上的线性函数相关。$s_j$ 取 1 或 -1。而 $\max$ 运算可以用 ReLU DNN 表示

<img src="/Users/xieyutong/Library/Application Support/typora-user-images/image-20200417200846815.png" alt="image-20200417200846815" style="zoom:50%;" />

利用分片线性函数在函数空间中的稠密性（可以任意逼近一个连续函数），所以 ReLU DNN 显然可以任意逼近任意连续函数，且深度不超过 $\left\lceil\log _{2}(n+1)\right\rceil+1$

其次，关于深度，作者给出了如下的定理，存在一类函数，当它可以用 $k+1$ 层、宽度为 $w$ 的 ReLU DNN 表示时，若用 $k'+1$ 层 ReLU DNN 表示（$k' <= k$)），则后者的结点数至少为 $\frac{1}{2} k' w^{\frac{k}{k'}}$. 这个 定理一定程度上说明了深度的好处。

最后，作者提出了一个复杂度的概念，$\text{comp}_{\mathcal{H}}(n, k, w)$，定义为可以用深度为 $k+1$，宽度最多 $w$ 的网络（该网络属于定义好的 $\mathcal{H}$）表示的 $n$ 元函数的分片数量。作者表示这是一个整数规划的的问题。在前人的一些工作中，设计出一类网络 $\mathcal{H}$，可以达到 $\left(\left\lfloor\left(\frac{w}{n}\right)\right\rfloor\right)^{(k-1) n}\left(\sum_{j=0}^{n}\left(\begin{array}{c}w \\ j\end{array}\right)\right)$. 在这一部分，作者提出了一类函数，这些函数可以用设计好的网络来表示。这类网络的定义如下：
$$
\mathrm{ZONOTOPE}_{k, w, m}^{n}\left[\mathbf{a}^{1}, \ldots, \mathbf{a}^{k}, \mathbf{b}^{1}, \ldots, \mathbf{b}^{m}\right]:=H_{\mathbf{a}^{1}, \ldots, \mathbf{a}^{k}} \circ \gamma_{Z\left(\mathbf{b}^{1}, \ldots, \mathbf{b}^{m}\right)}
$$
其中，$$H_{\mathbf{a}^{1}, \ldots, \mathbf{a}^{k}}:=h_{\mathbf{a}^{k}} \circ h_{\mathbf{a}^{k-1}} \circ \ldots \circ h_{\mathbf{a}^{1}}$$ 是多个一元函数的复合，$\gamma_{Z\left(\mathbf{b}^{1}, \ldots, \mathbf{b}^{m}\right)}(\mathbf{r})=\max _{\mathbf{x} \in Z\left(\mathbf{b}^{1}, \ldots, \mathbf{b}^{m}\right)}\langle\mathbf{r}, \mathbf{x}\rangle$，$Z$  表示的是一个多面体。这样的一类函数，$\gamma$ 部分用 2 层的 ReLU 网络表示，$H$ 根据之前的定理可以用 $k+1$ 层网络表示，利用函数复合和网络复合的关系，可以证明 $\mathrm{ZONOTOPE}_{k, w, m}^{n}\left[\mathbf{a}^{1}, \ldots, \mathbf{a}^{k}, \mathbf{b}^{1}, \ldots, \mathbf{b}^{m}\right]$ 可以用 $k+2$ 层网络来表示，结点数为 $2m + wk$，$2m$ 和 $\gamma$ 有关，$wk$ 和 $H$ 有关。这类函数的分片数量为 
$$
\left(\sum_{i=0}^{n-1}\left(\begin{array}{c}
m-1 \\
i
\end{array}\right)\right) w^{k}
$$


总结：这些文章，虽然或多或少都涉及到了网络表达的上下界，但是很显然（特别是第一、二、四篇论文），他们讨论的都是特定的网络结构，比如第一篇中隐层的设计 $(x, y+\ell(x))$，第二篇中需要利用 $x^2,xy$ 这些基本函数的逼近，结合泰勒展开，第四篇中需要用到 $\max$ 运算对应的网络结构。对于 $\gamma$ 函数，也是用了特定的网络结构。这些理论讨论设计的网络，和实际应用中的网络有很大的区别。



新下载的文献

1. Understanding deep convolutional networks，2016
2. Deep vs. shallow networks: An approximation theory perspective，2016，网络深度，逼近论。重点
3. Universal function approximation by deep neural nets with bounded width and relu activations，2017，网络深度，表达能力
4. Learning real and boolean functions: When is deep better than shallow，2016，网络深度，表达能力
5. Representation beneﬁts of deep feedforward networks，2015
6. Neural networks and rational functions，2017
7. Deep learning and the information bottleneck principle，2015，信息论
8. On the number of linear regions of deep neural networks，2014
9. Approximation theory of the MLP model in neural networks，1999。重点
10. Why deep neural networks for function approximation，2016
11. Approximation capabilities of multilayer feedforward networks，1991。重点
12. Bounding and counting linear regions of deep neural networks. 2017。重点



list

On the number of linear regions of deep neural networks

The power of depth for feedforward neural networks

Representation beneﬁts of deep feedforward networks

On the expressive power of deep neural networks

Notes on the number of linear regions of deep neural networks